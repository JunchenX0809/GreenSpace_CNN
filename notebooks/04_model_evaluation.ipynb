{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04. Model Evaluation - GreenSpace CNN\n",
        "\n",
        "Comprehensive evaluation of the trained multitask CNN:\n",
        "\n",
        "## Evaluation Components\n",
        "1. **Per-task metrics**: Regression (MAE, RÂ²), Binary (F1, AUC), Categorical (Accuracy)\n",
        "2. **Model interpretability**: Feature importance, activation maps\n",
        "3. **Error analysis**: Failure cases, confusion matrices\n",
        "4. **Spatial analysis**: Geographic patterns in predictions\n",
        "5. **Comparison**: Different architectures and baselines\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded val split: 10\n",
            "Binary labels: ['sports_field_p', 'multipurpose_open_area_p', 'childrens_playground_p', 'water_feature_p', 'gardens_p', 'walking_paths_p', 'built_structures_p']\n",
            "Shade cols   : ['shade_p_none', 'shade_p_some', 'shade_p_abundant']\n",
            "Score cols   : ['score_p_1', 'score_p_2', 'score_p_3', 'score_p_4', 'score_p_5']\n"
          ]
        }
      ],
      "source": [
        "# Setup: imports, paths, and dataframes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "\n",
        "# Paths\n",
        "train_csv = Path('../data/processed/splits/train.csv')\n",
        "val_csv   = Path('../data/processed/splits/val.csv')\n",
        "test_csv  = Path('../data/processed/splits/test.csv')\n",
        "\n",
        "assert val_csv.exists(), 'Missing val.csv split manifest. Run 02 first.'\n",
        "\n",
        "val_df   = pd.read_csv(val_csv)\n",
        "print('Loaded val split:', len(val_df))\n",
        "\n",
        "# Identify label columns (match training)\n",
        "binary_cols = [c for c in val_df.columns if c.endswith('_p') and not c.startswith(('shade_p_', 'score_p_'))]\n",
        "shade_cols  = [c for c in val_df.columns if c.startswith('shade_p_')]\n",
        "score_cols  = [c for c in val_df.columns if c.startswith('score_p_')]\n",
        "\n",
        "print('Binary labels:', binary_cols)\n",
        "print('Shade cols   :', shade_cols)\n",
        "print('Score cols   :', score_cols)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation dataset ready: 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-12 22:26:17.102473: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4\n",
            "2025-10-12 22:26:17.102661: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
            "2025-10-12 22:26:17.102668: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
            "2025-10-12 22:26:17.103011: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2025-10-12 22:26:17.103039: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        }
      ],
      "source": [
        "# Build val dataset (no augmentation)\n",
        "IMG_SIZE = (512, 512)\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "def decode_image(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "paths = val_df['image_path'].astype(str).tolist()\n",
        "\n",
        "ds_paths = tf.data.Dataset.from_tensor_slices(paths)\n",
        "ds_imgs = ds_paths.map(decode_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# labels\n",
        "y_bin = val_df[binary_cols].astype(np.float32).values\n",
        "shade_cols = [c for c in val_df.columns if c.startswith('shade_p_')]\n",
        "score_cols = [c for c in val_df.columns if c.startswith('score_p_')]\n",
        "y_shade = val_df[shade_cols].astype(np.float32).values\n",
        "y_score = val_df[score_cols].astype(np.float32).values\n",
        "\n",
        "ds_labels = tf.data.Dataset.from_tensor_slices({\n",
        "    'bin_head': y_bin,\n",
        "    'shade_head': y_shade,\n",
        "    'score_head': y_score,\n",
        "})\n",
        "val_ds = tf.data.Dataset.zip((ds_imgs, ds_labels)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "print('Validation dataset ready:', len(paths))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model from ../models/best.keras\n"
          ]
        }
      ],
      "source": [
        "# Load best model\n",
        "model_path = Path('../models/best.keras')\n",
        "assert model_path.exists(), 'Missing best.keras. Train (03) first.'\n",
        "model = tf.keras.models.load_model(str(model_path))\n",
        "print('Loaded model from', model_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-10-12 22:26:25.941941: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calibrated thresholds saved to ../data/processed/thresholds.json\n",
            "Binary metrics (val):\n",
            "  sports_field: P=0.00 R=0.00 F1=0.00 @t=0.05\n",
            "  multipurpose_open_area: P=0.00 R=0.00 F1=0.00 @t=0.05\n",
            "  childrens_playground: P=0.00 R=0.00 F1=0.00 @t=0.05\n",
            "  water_feature: P=0.00 R=0.00 F1=0.00 @t=0.05\n",
            "  gardens: P=0.00 R=0.00 F1=0.00 @t=0.05\n",
            "  walking_paths: P=0.00 R=0.00 F1=0.00 @t=0.05\n",
            "  built_structures: P=0.00 R=0.00 F1=0.00 @t=0.05\n",
            "Shade val accuracy: 0.60\n",
            "Score val accuracy: 0.10; MAE(exp): nan\n"
          ]
        }
      ],
      "source": [
        "# Predict on validation\n",
        "pred_bin, pred_shade, pred_score = model.predict(val_ds, verbose=0)\n",
        "\n",
        "# Build ground-truth arrays from val_df\n",
        "bin_names = [c[:-2] for c in binary_cols]\n",
        "bin_names = [c for c in bin_names if c in val_df.columns]\n",
        "y_bin_true = val_df[bin_names].astype(int).values\n",
        "\n",
        "y_shade_true = val_df['shade_class'].astype(int).values if 'shade_class' in val_df.columns else None\n",
        "y_score_true = val_df['score_class'].astype(int).values if 'score_class' in val_df.columns else None\n",
        "\n",
        "# Calibration: pick threshold per binary label by maximizing F1 on val\n",
        "import json\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "\n",
        "thresholds = {}\n",
        "metrics_bin = {}\n",
        "ths = np.linspace(0.05, 0.95, 19)\n",
        "for i, name in enumerate(bin_names):\n",
        "    best_f1, best_t = -1.0, 0.5\n",
        "    y_prob = pred_bin[:, i]\n",
        "    y_true = y_bin_true[:, i]\n",
        "    for t in ths:\n",
        "        y_hat = (y_prob >= t).astype(int)\n",
        "        p, r, f1, _ = precision_recall_fscore_support(y_true, y_hat, average='binary', zero_division=0)\n",
        "        if f1 > best_f1:\n",
        "            best_f1, best_t = f1, t\n",
        "    thresholds[name] = float(best_t)\n",
        "    # report at chosen threshold\n",
        "    y_hat = (y_prob >= best_t).astype(int)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_hat, average='binary', zero_division=0)\n",
        "    metrics_bin[name] = {'precision': float(p), 'recall': float(r), 'f1': float(f1)}\n",
        "\n",
        "# Shade/Score metrics on val (argmax)\n",
        "metrics_val = {'binary': metrics_bin, 'shade': {}, 'score': {}}\n",
        "if y_shade_true is not None:\n",
        "    shade_pred_class = pred_shade.argmax(axis=1)\n",
        "    from sklearn.metrics import accuracy_score\n",
        "    acc_shade = accuracy_score(y_shade_true, shade_pred_class)\n",
        "    metrics_val['shade']['accuracy'] = float(acc_shade)\n",
        "\n",
        "if y_score_true is not None:\n",
        "    score_pred_class = pred_score.argmax(axis=1) + 1\n",
        "    acc_score = accuracy_score(y_score_true, score_pred_class)\n",
        "    # expected score MAE\n",
        "    classes = np.arange(1, pred_score.shape[1] + 1, dtype=np.float32)\n",
        "    score_expected = (pred_score * classes).sum(axis=1)\n",
        "    y_true = y_score_true.astype(np.float32)\n",
        "    valid = (~np.isnan(score_expected)) & (~np.isnan(y_true))\n",
        "    mae_score = float(np.mean(np.abs(score_expected[valid] - y_true[valid]))) if valid.sum() > 0 else float('nan')\n",
        "    metrics_val['score']['accuracy'] = float(acc_score)\n",
        "    metrics_val['score']['mae_expected'] = float(mae_score)\n",
        "\n",
        "# Save thresholds\n",
        "thr_path = Path('../data/processed/thresholds.json')\n",
        "thr_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(thr_path, 'w') as f:\n",
        "    json.dump({'thresholds': thresholds}, f, indent=2)\n",
        "\n",
        "print('Calibrated thresholds saved to', thr_path)\n",
        "print('Binary metrics (val):')\n",
        "for k, v in metrics_bin.items():\n",
        "    print(f\"  {k}: P={v['precision']:.2f} R={v['recall']:.2f} F1={v['f1']:.2f} @t={thresholds[k]:.2f}\")\n",
        "if 'accuracy' in metrics_val.get('shade', {}):\n",
        "    print(f\"Shade val accuracy: {metrics_val['shade']['accuracy']:.2f}\")\n",
        "if 'accuracy' in metrics_val.get('score', {}):\n",
        "    print(f\"Score val accuracy: {metrics_val['score']['accuracy']:.2f}; MAE(exp): {metrics_val['score']['mae_expected']:.2f}\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
