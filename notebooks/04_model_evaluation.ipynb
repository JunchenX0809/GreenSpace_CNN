{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 04. Model Evaluation - GreenSpace CNN\n",
        "\n",
        "Comprehensive evaluation of the trained multitask CNN:\n",
        "\n",
        "## Evaluation Components\n",
        "1. **Per-task metrics**: Regression (MAE, RÂ²), Binary (F1, AUC), Categorical (Accuracy)\n",
        "2. **Model interpretability**: Feature importance, activation maps\n",
        "3. **Error analysis**: Failure cases, confusion matrices\n",
        "4. **Spatial analysis**: Geographic patterns in predictions\n",
        "5. **Comparison**: Different architectures and baselines\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/starsrain/2025_codeProject/GreenSpace_CNN/.venv/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(np, \"object\"):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded splits: {'train': 1896, 'val': 632, 'test': 632}\n",
            "Binary prob cols: ['sports_field_p', 'multipurpose_open_area_p', 'children_s_playground_p', 'water_feature_p', 'gardens_p', 'walking_paths_p', 'built_structures_p', 'parking_lots_p']\n",
            "Class cols       : ['shade_class', 'score_class', 'veg_class']\n"
          ]
        }
      ],
      "source": [
        "# Setup: imports, paths, and dataframes\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "\n",
        "# Evaluate on the saved split manifests (created in 02_data_preprocessing.ipynb)\n",
        "splits_dir = Path('../data/processed/splits')\n",
        "train_csv = splits_dir / 'train.csv'\n",
        "val_csv   = splits_dir / 'val.csv'\n",
        "test_csv  = splits_dir / 'test.csv'\n",
        "\n",
        "for p in [train_csv, val_csv, test_csv]:\n",
        "    assert p.exists(), f\"Missing split manifest: {p} (run 02 first)\"\n",
        "\n",
        "train_df = pd.read_csv(train_csv)\n",
        "val_df   = pd.read_csv(val_csv)\n",
        "test_df  = pd.read_csv(test_csv)\n",
        "\n",
        "print('Loaded splits:', {\"train\": len(train_df), \"val\": len(val_df), \"test\": len(test_df)})\n",
        "\n",
        "# Binary labels are stored as probabilities in *_p columns\n",
        "binary_cols = [c for c in train_df.columns if c.endswith('_p')]\n",
        "assert binary_cols, 'No *_p binary prob cols found in split manifests'\n",
        "\n",
        "# Class targets (int columns)\n",
        "for df_name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:\n",
        "    for c in ['shade_class', 'score_class', 'veg_class', 'image_path']:\n",
        "        assert c in df.columns, f\"Missing {c} in {df_name}.csv\"\n",
        "\n",
        "print('Binary prob cols:', binary_cols)\n",
        "print('Class cols       :', ['shade_class', 'score_class', 'veg_class'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Datasets ready: {'train': 1896, 'val': 632, 'test': 632}\n"
          ]
        }
      ],
      "source": [
        "# Build datasets (no augmentation)\n",
        "IMG_SIZE = (512, 512)\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "NUM_SHADE = 2\n",
        "NUM_SCORE = 5\n",
        "NUM_VEG = 5\n",
        "\n",
        "def decode_image(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "def make_ds(df):\n",
        "    paths = df['image_path'].astype(str).tolist()\n",
        "\n",
        "    ds_paths = tf.data.Dataset.from_tensor_slices(paths)\n",
        "    ds_imgs = ds_paths.map(decode_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    # labels (match 03 training)\n",
        "    y_bin = df[binary_cols].fillna(0.0).astype(np.float32).values\n",
        "\n",
        "    y_shade = df['shade_class'].fillna(0).astype(np.int32).values\n",
        "\n",
        "    # score/veg are stored as 1..5 in the manifest; training uses 0..4\n",
        "    y_score = df['score_class'].fillna(1).astype(np.int32).values - 1\n",
        "    y_veg   = df['veg_class'].fillna(1).astype(np.int32).values - 1\n",
        "\n",
        "    # clip defensively\n",
        "    y_shade = np.clip(y_shade, 0, NUM_SHADE - 1)\n",
        "    y_score = np.clip(y_score, 0, NUM_SCORE - 1)\n",
        "    y_veg   = np.clip(y_veg,   0, NUM_VEG - 1)\n",
        "\n",
        "    ds_labels = tf.data.Dataset.from_tensor_slices({\n",
        "        'bin_head': y_bin,\n",
        "        'shade_head': y_shade,\n",
        "        'score_head': y_score,\n",
        "        'veg_head': y_veg,\n",
        "    })\n",
        "\n",
        "    return tf.data.Dataset.zip((ds_imgs, ds_labels)).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_ds = make_ds(train_df)\n",
        "val_ds   = make_ds(val_df)\n",
        "test_ds  = make_ds(test_df)\n",
        "\n",
        "print('Datasets ready:', {\"train\": len(train_df), \"val\": len(val_df), \"test\": len(test_df)})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using RUN_DIR = ../models/runs/20260204_203132\n",
            "Loaded model from ../models/runs/20260204_203132/final_20260204_203132.keras\n"
          ]
        }
      ],
      "source": [
        "# Load a trained model\n",
        "# Preferred layout: models/runs/<RUN_TAG>/final_<RUN_TAG>.keras\n",
        "# Fallback: ../models (legacy flat layout).\n",
        "\n",
        "runs_root = Path('../models/runs')\n",
        "\n",
        "# Default: pick the most recently modified run folder, unless you override RUN_DIR manually.\n",
        "RUN_DIR = globals().get('RUN_DIR', None)\n",
        "if RUN_DIR is None:\n",
        "    if runs_root.exists():\n",
        "        run_dirs = [p for p in runs_root.iterdir() if p.is_dir()]\n",
        "        run_dirs = sorted(run_dirs, key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "        RUN_DIR = run_dirs[0] if run_dirs else (runs_root / 'REPLACE_WITH_RUN_TAG')\n",
        "    else:\n",
        "        RUN_DIR = runs_root / 'REPLACE_WITH_RUN_TAG'\n",
        "\n",
        "RUN_DIR = Path(RUN_DIR)\n",
        "print('Using RUN_DIR =', RUN_DIR)\n",
        "\n",
        "candidates = []\n",
        "\n",
        "# 1) Preferred: run-scoped directory\n",
        "if RUN_DIR.exists():\n",
        "    candidates += sorted(RUN_DIR.glob('final*.keras'))\n",
        "    if not candidates:\n",
        "        candidates += sorted(RUN_DIR.glob('best*.keras'))\n",
        "\n",
        "# 2) Fallback: legacy flat ../models directory\n",
        "if not candidates:\n",
        "    models_dir = Path('../models')\n",
        "    candidates += sorted(models_dir.glob('final_*.keras'))\n",
        "    if not candidates:\n",
        "        candidates += sorted(models_dir.glob('best*.keras'))\n",
        "\n",
        "assert candidates, (\n",
        "    f\"No model .keras found. Checked RUN_DIR={RUN_DIR} and ../models. \"\n",
        "    f\"(Expected e.g. final*.keras or best*.keras)\"\n",
        ")\n",
        "\n",
        "model_path = candidates[-1]\n",
        "model = tf.keras.models.load_model(str(model_path))\n",
        "print('Loaded model from', model_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Label Loss Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model run tag: 20260204_203132\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loss</th>\n",
              "      <th>bin_head_loss</th>\n",
              "      <th>shade_head_loss</th>\n",
              "      <th>score_head_loss</th>\n",
              "      <th>veg_head_loss</th>\n",
              "      <th>bin_head_binary_accuracy</th>\n",
              "      <th>shade_head_sparse_categorical_accuracy</th>\n",
              "      <th>score_head_sparse_categorical_accuracy</th>\n",
              "      <th>veg_head_sparse_categorical_accuracy</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>split</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>train</th>\n",
              "      <td>2.6311</td>\n",
              "      <td>0.3384</td>\n",
              "      <td>0.4969</td>\n",
              "      <td>0.9838</td>\n",
              "      <td>0.8120</td>\n",
              "      <td>0.8501</td>\n",
              "      <td>0.7632</td>\n",
              "      <td>0.5765</td>\n",
              "      <td>0.6688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>val</th>\n",
              "      <td>3.3585</td>\n",
              "      <td>0.3513</td>\n",
              "      <td>0.5446</td>\n",
              "      <td>1.3893</td>\n",
              "      <td>1.0733</td>\n",
              "      <td>0.8453</td>\n",
              "      <td>0.7421</td>\n",
              "      <td>0.4193</td>\n",
              "      <td>0.5585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>test</th>\n",
              "      <td>3.5315</td>\n",
              "      <td>0.3445</td>\n",
              "      <td>0.6356</td>\n",
              "      <td>1.4210</td>\n",
              "      <td>1.1304</td>\n",
              "      <td>0.8443</td>\n",
              "      <td>0.7104</td>\n",
              "      <td>0.4241</td>\n",
              "      <td>0.5633</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         loss  bin_head_loss  shade_head_loss  score_head_loss  veg_head_loss  \\\n",
              "split                                                                           \n",
              "train  2.6311         0.3384           0.4969           0.9838         0.8120   \n",
              "val    3.3585         0.3513           0.5446           1.3893         1.0733   \n",
              "test   3.5315         0.3445           0.6356           1.4210         1.1304   \n",
              "\n",
              "       bin_head_binary_accuracy  shade_head_sparse_categorical_accuracy  \\\n",
              "split                                                                     \n",
              "train                    0.8501                                  0.7632   \n",
              "val                      0.8453                                  0.7421   \n",
              "test                     0.8443                                  0.7104   \n",
              "\n",
              "       score_head_sparse_categorical_accuracy  \\\n",
              "split                                           \n",
              "train                                  0.5765   \n",
              "val                                    0.4193   \n",
              "test                                   0.4241   \n",
              "\n",
              "       veg_head_sparse_categorical_accuracy  \n",
              "split                                        \n",
              "train                                0.6688  \n",
              "val                                  0.5585  \n",
              "test                                 0.5633  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: per-head losses are cross-entropy terms (not directly comparable across heads).\n",
            "Best practice: compare each head across runs + compare train vs val for that head (over/underfitting).\n"
          ]
        }
      ],
      "source": [
        "# Monitoring: per-head losses + metrics (train / val / test)\n",
        "# This is the cleanest way to discuss \"which head is improving\" across runs.\n",
        "\n",
        "# Ensure the loaded model has the same losses/metrics as training.\n",
        "# (Optimizer choice does not matter for evaluation, but compile is required for evaluate(..., return_dict=True).)\n",
        "losses = {\n",
        "    'bin_head': 'binary_crossentropy',\n",
        "    'shade_head': 'sparse_categorical_crossentropy',\n",
        "    'score_head': 'sparse_categorical_crossentropy',\n",
        "    'veg_head': 'sparse_categorical_crossentropy',\n",
        "}\n",
        "metrics = {\n",
        "    'bin_head': ['binary_accuracy'],\n",
        "    'shade_head': ['sparse_categorical_accuracy'],\n",
        "    'score_head': ['sparse_categorical_accuracy'],\n",
        "    'veg_head': ['sparse_categorical_accuracy'],\n",
        "}\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss=losses, metrics=metrics)\n",
        "\n",
        "# Infer run tag from the run folder (models/runs/<RUN_TAG>/...) or filename (final_<RUN_TAG>.keras)\n",
        "run_tag = None\n",
        "try:\n",
        "    p = Path(model_path)\n",
        "    # If using models/runs/<RUN_TAG>/..., prefer the folder name\n",
        "    if 'runs' in p.parts:\n",
        "        runs_idx = p.parts.index('runs')\n",
        "        if runs_idx + 1 < len(p.parts):\n",
        "            run_tag = p.parts[runs_idx + 1]\n",
        "    # Fallback: parse from filename final_<RUN_TAG>.keras\n",
        "    if run_tag is None:\n",
        "        name = p.name\n",
        "        if name.startswith('final_') and name.endswith('.keras'):\n",
        "            run_tag = name[len('final_'):-len('.keras')]\n",
        "except Exception:\n",
        "    pass\n",
        "print('Model run tag:', run_tag)\n",
        "\n",
        "def eval_split(split_name, ds):\n",
        "    d = model.evaluate(ds, verbose=0, return_dict=True)\n",
        "    d['split'] = split_name\n",
        "    return d\n",
        "\n",
        "rows = [\n",
        "    eval_split('train', train_ds),\n",
        "    eval_split('val',   val_ds),\n",
        "    eval_split('test',  test_ds),\n",
        "]\n",
        "mon = pd.DataFrame(rows).set_index('split')\n",
        "\n",
        "# Keep this table small and report-friendly.\n",
        "keep = [\n",
        "    'loss',\n",
        "    'bin_head_loss', 'shade_head_loss', 'score_head_loss', 'veg_head_loss',\n",
        "    'bin_head_binary_accuracy',\n",
        "    'shade_head_sparse_categorical_accuracy',\n",
        "    'score_head_sparse_categorical_accuracy',\n",
        "    'veg_head_sparse_categorical_accuracy',\n",
        "]\n",
        "\n",
        "# Some keys may be absent depending on how the model was saved/loaded.\n",
        "keep = [k for k in keep if k in mon.columns]\n",
        "\n",
        "display(mon[keep].round(4))\n",
        "print('Note: per-head losses are cross-entropy terms (not directly comparable across heads).')\n",
        "print('Best practice: compare each head across runs + compare train vs val for that head (over/underfitting).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save Loss Monitoring Artifact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved monitoring table to /Users/starsrain/2025_codeProject/GreenSpace_CNN/monitoring_output/loss_monitor_20260204_203132.csv\n"
          ]
        }
      ],
      "source": [
        "# Save monitoring table\n",
        "from datetime import datetime\n",
        "\n",
        "out_dir = (Path('../monitoring_output')).resolve()\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "tag = run_tag or datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "out_path = out_dir / f\"loss_monitor_{tag}.csv\"\n",
        "\n",
        "# Save only the compact report columns\n",
        "mon[keep].to_csv(out_path)\n",
        "print('Saved monitoring table to', out_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Threshold Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tuning thresholds on VAL for labels: ['sports_field', 'multipurpose_open_area', 'children_s_playground', 'water_feature', 'gardens', 'walking_paths', 'built_structures', 'parking_lots']\n",
            "\n",
            "Top thresholds by best_f1 (VAL):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>best_threshold</th>\n",
              "      <th>best_f1</th>\n",
              "      <th>best_precision</th>\n",
              "      <th>best_recall</th>\n",
              "      <th>pos_rate</th>\n",
              "      <th>n_pos</th>\n",
              "      <th>n</th>\n",
              "      <th>note</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>multipurpose_open_area</td>\n",
              "      <td>0.272524</td>\n",
              "      <td>0.935743</td>\n",
              "      <td>0.915521</td>\n",
              "      <td>0.956879</td>\n",
              "      <td>0.770570</td>\n",
              "      <td>487</td>\n",
              "      <td>632</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>walking_paths</td>\n",
              "      <td>0.332493</td>\n",
              "      <td>0.907937</td>\n",
              "      <td>0.871951</td>\n",
              "      <td>0.947020</td>\n",
              "      <td>0.716772</td>\n",
              "      <td>453</td>\n",
              "      <td>632</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>built_structures</td>\n",
              "      <td>0.450735</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.798354</td>\n",
              "      <td>0.801653</td>\n",
              "      <td>0.382911</td>\n",
              "      <td>242</td>\n",
              "      <td>632</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>parking_lots</td>\n",
              "      <td>0.451296</td>\n",
              "      <td>0.712766</td>\n",
              "      <td>0.653659</td>\n",
              "      <td>0.783626</td>\n",
              "      <td>0.270570</td>\n",
              "      <td>171</td>\n",
              "      <td>632</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sports_field</td>\n",
              "      <td>0.431477</td>\n",
              "      <td>0.712251</td>\n",
              "      <td>0.706215</td>\n",
              "      <td>0.718391</td>\n",
              "      <td>0.275316</td>\n",
              "      <td>174</td>\n",
              "      <td>632</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>children_s_playground</td>\n",
              "      <td>0.217674</td>\n",
              "      <td>0.426230</td>\n",
              "      <td>0.393939</td>\n",
              "      <td>0.464286</td>\n",
              "      <td>0.132911</td>\n",
              "      <td>84</td>\n",
              "      <td>632</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>water_feature</td>\n",
              "      <td>0.250731</td>\n",
              "      <td>0.415663</td>\n",
              "      <td>0.312217</td>\n",
              "      <td>0.621622</td>\n",
              "      <td>0.175633</td>\n",
              "      <td>111</td>\n",
              "      <td>632</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>gardens</td>\n",
              "      <td>0.083024</td>\n",
              "      <td>0.163090</td>\n",
              "      <td>0.093137</td>\n",
              "      <td>0.655172</td>\n",
              "      <td>0.045886</td>\n",
              "      <td>29</td>\n",
              "      <td>632</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    label  best_threshold   best_f1  best_precision  \\\n",
              "0  multipurpose_open_area        0.272524  0.935743        0.915521   \n",
              "1           walking_paths        0.332493  0.907937        0.871951   \n",
              "2        built_structures        0.450735  0.800000        0.798354   \n",
              "3            parking_lots        0.451296  0.712766        0.653659   \n",
              "4            sports_field        0.431477  0.712251        0.706215   \n",
              "5   children_s_playground        0.217674  0.426230        0.393939   \n",
              "6           water_feature        0.250731  0.415663        0.312217   \n",
              "7                 gardens        0.083024  0.163090        0.093137   \n",
              "\n",
              "   best_recall  pos_rate  n_pos    n note  \n",
              "0     0.956879  0.770570    487  632       \n",
              "1     0.947020  0.716772    453  632       \n",
              "2     0.801653  0.382911    242  632       \n",
              "3     0.783626  0.270570    171  632       \n",
              "4     0.718391  0.275316    174  632       \n",
              "5     0.464286  0.132911     84  632       \n",
              "6     0.621622  0.175633    111  632       \n",
              "7     0.655172  0.045886     29  632       "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "overall (VAL) over definable labels: F1=0.634 | P=0.593 | R=0.744\n",
            "Saved thresholds to /Users/starsrain/2025_codeProject/GreenSpace_CNN/monitoring_output/thresholds_20260204_203132.csv\n"
          ]
        }
      ],
      "source": [
        "# Threshold tuning (binary heads): tune on VAL to maximize F1, then apply to TEST\n",
        "# This does NOT require retraining.\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from datetime import datetime\n",
        "\n",
        "# 1) Predict on validation\n",
        "pred_bin_val, _, _, _ = model.predict(val_ds, verbose=0)\n",
        "\n",
        "# Align y_true (val) to the same binary label order used for probabilities\n",
        "bin_names = [c[:-2] for c in binary_cols]\n",
        "hard_bin_names_val = [c for c in bin_names if c in val_df.columns]\n",
        "\n",
        "if hard_bin_names_val:\n",
        "    y_bin_val_true = val_df[hard_bin_names_val].fillna(0).astype(int).values\n",
        "    pred_bin_val_aligned = np.stack([pred_bin_val[:, bin_names.index(n)] for n in hard_bin_names_val], axis=1)\n",
        "    label_names = hard_bin_names_val\n",
        "else:\n",
        "    # Fallback: if hard 0/1 columns are not present, create pseudo-hard labels from *_p\n",
        "    y_bin_val_true = (val_df[binary_cols].fillna(0.0).astype(np.float32).values >= 0.5).astype(int)\n",
        "    pred_bin_val_aligned = pred_bin_val\n",
        "    label_names = bin_names\n",
        "\n",
        "print('Tuning thresholds on VAL for labels:', label_names)\n",
        "\n",
        "\n",
        "def tune_thresholds_f1(y_true_mat, y_prob_mat, label_names, min_pos=1):\n",
        "    \"\"\"Tune per-label thresholds on validation to maximize F1.\n",
        "\n",
        "    - Uses precision_recall_curve to generate candidate thresholds.\n",
        "    - Skips labels with <2 classes in y_true.\n",
        "    - Tie-break: choose the smallest threshold among max-F1 candidates (recall-friendly).\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for i, name in enumerate(label_names):\n",
        "        y_true = np.asarray(y_true_mat[:, i]).astype(int)\n",
        "        y_prob = np.asarray(y_prob_mat[:, i]).astype(float)\n",
        "\n",
        "        n_pos = int(y_true.sum())\n",
        "        n = int(len(y_true))\n",
        "        pos_rate = float(y_true.mean()) if n else float('nan')\n",
        "\n",
        "        if np.unique(y_true).size < 2 or n_pos < min_pos:\n",
        "            rows.append({\n",
        "                'label': name,\n",
        "                'best_threshold': np.nan,\n",
        "                'best_f1': np.nan,\n",
        "                'best_precision': np.nan,\n",
        "                'best_recall': np.nan,\n",
        "                'pos_rate': pos_rate,\n",
        "                'n_pos': n_pos,\n",
        "                'n': n,\n",
        "                'note': 'single-class' if np.unique(y_true).size < 2 else 'too-few-positives',\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        precision, recall, thresholds = precision_recall_curve(y_true, y_prob)\n",
        "        # thresholds has length = len(precision)-1\n",
        "        if thresholds.size == 0:\n",
        "            rows.append({\n",
        "                'label': name,\n",
        "                'best_threshold': np.nan,\n",
        "                'best_f1': np.nan,\n",
        "                'best_precision': np.nan,\n",
        "                'best_recall': np.nan,\n",
        "                'pos_rate': pos_rate,\n",
        "                'n_pos': n_pos,\n",
        "                'n': n,\n",
        "                'note': 'no-thresholds',\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        p = precision[:-1]\n",
        "        r = recall[:-1]\n",
        "        t = thresholds\n",
        "        f1 = (2 * p * r) / (p + r + 1e-12)\n",
        "\n",
        "        best_f1 = float(np.max(f1))\n",
        "        best_idxs = np.flatnonzero(f1 == best_f1)\n",
        "        best_idx = int(best_idxs[0])  # smallest threshold among ties\n",
        "\n",
        "        rows.append({\n",
        "            'label': name,\n",
        "            'best_threshold': float(t[best_idx]),\n",
        "            'best_f1': best_f1,\n",
        "            'best_precision': float(p[best_idx]),\n",
        "            'best_recall': float(r[best_idx]),\n",
        "            'pos_rate': pos_rate,\n",
        "            'n_pos': n_pos,\n",
        "            'n': n,\n",
        "            'note': '',\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    return df\n",
        "\n",
        "\n",
        "thresh_df = tune_thresholds_f1(y_bin_val_true, pred_bin_val_aligned, label_names)\n",
        "\n",
        "# Save for reuse in later cells\n",
        "best_thresholds = {\n",
        "    row['label']: float(row['best_threshold'])\n",
        "    for _, row in thresh_df.iterrows()\n",
        "    if np.isfinite(row['best_threshold'])\n",
        "}\n",
        "\n",
        "print('\\nTop thresholds by best_f1 (VAL):')\n",
        "display(thresh_df.sort_values('best_f1', ascending=False).reset_index(drop=True))\n",
        "\n",
        "_defined = thresh_df['best_f1'].notna()\n",
        "if _defined.any():\n",
        "    print(\n",
        "        f\"overall (VAL) over definable labels: \"\n",
        "        f\"F1={float(thresh_df.loc[_defined, 'best_f1'].mean()):.3f} | \"\n",
        "        f\"P={float(thresh_df.loc[_defined, 'best_precision'].mean()):.3f} | \"\n",
        "        f\"R={float(thresh_df.loc[_defined, 'best_recall'].mean()):.3f}\"\n",
        "    )\n",
        "else:\n",
        "    print('overall (VAL): NA (no definable labels)')\n",
        "\n",
        "# Optional: save thresholds to monitoring_output\n",
        "out_dir = Path('../monitoring_output').resolve()\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "_tag = globals().get('run_tag', None)\n",
        "if _tag is None:\n",
        "    _tag = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "out_path = out_dir / f\"thresholds_{_tag}.csv\"\n",
        "thresh_df.to_csv(out_path, index=False)\n",
        "print('Saved thresholds to', out_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Binary (threshold=0.5) ---\n",
            "sports_field             P=0.74 R=0.69 F1=0.71\n",
            "multipurpose_open_area   P=0.95 R=0.85 F1=0.89\n",
            "children_s_playground    P=0.33 R=0.05 F1=0.08\n",
            "water_feature            P=0.56 R=0.04 F1=0.07\n",
            "gardens                  P=0.00 R=0.00 F1=0.00\n",
            "walking_paths            P=0.90 R=0.90 F1=0.90\n",
            "built_structures         P=0.84 R=0.71 F1=0.77\n",
            "parking_lots             P=0.68 R=0.72 F1=0.70\n",
            "overall F1 (threshold=0.5) = 0.516\n",
            "--- Binary (AUC) ---\n",
            "sports_field             ROC_AUC=0.899 PR_AUC=0.813\n",
            "multipurpose_open_area   ROC_AUC=0.919 PR_AUC=0.972\n",
            "children_s_playground    ROC_AUC=0.713 PR_AUC=0.225\n",
            "water_feature            ROC_AUC=0.648 PR_AUC=0.328\n",
            "gardens                  ROC_AUC=0.671 PR_AUC=0.066\n",
            "walking_paths            ROC_AUC=0.911 PR_AUC=0.965\n",
            "built_structures         ROC_AUC=0.911 PR_AUC=0.876\n",
            "parking_lots             ROC_AUC=0.895 PR_AUC=0.755\n",
            "overall ROC_AUC=0.821 overall PR_AUC=0.625\n",
            "--- Binary (tuned thresholds from val; optimize F1) ---\n",
            "sports_field             thr=0.431 P=0.70 R=0.74 F1=0.72\n",
            "multipurpose_open_area   thr=0.273 P=0.92 R=0.95 F1=0.93\n",
            "children_s_playground    thr=0.218 P=0.27 R=0.34 F1=0.30\n",
            "water_feature            thr=0.251 P=0.29 R=0.49 F1=0.36\n",
            "gardens                  thr=0.083 P=0.08 R=0.58 F1=0.14\n",
            "walking_paths            thr=0.332 P=0.88 R=0.95 F1=0.91\n",
            "built_structures         thr=0.451 P=0.83 R=0.75 F1=0.79\n",
            "parking_lots             thr=0.451 P=0.67 R=0.77 F1=0.71\n",
            "overall F1=0.608 (over tuned/defined labels)\n",
            "--- Shade / Score / Veg ---\n",
            "Shade accuracy: 0.710\n",
            "Score accuracy: 0.424\n",
            "Veg   accuracy: 0.563\n",
            "Score MAE (expected value): 0.746\n",
            "Veg   MAE (expected value): 0.568\n"
          ]
        }
      ],
      "source": [
        "# Predict on test\n",
        "pred_bin, pred_shade, pred_score, pred_veg = model.predict(test_ds, verbose=0)\n",
        "\n",
        "# Ground truth\n",
        "# For binaries, prefer hard 0/1 columns if present (e.g. sports_field), else threshold *_p at 0.5\n",
        "bin_names = [c[:-2] for c in binary_cols]\n",
        "hard_bin_names = [c for c in bin_names if c in test_df.columns]\n",
        "\n",
        "if hard_bin_names:\n",
        "    y_bin_true = test_df[hard_bin_names].fillna(0).astype(int).values\n",
        "    # align pred_bin columns to hard_bin_names order\n",
        "    pred_bin_aligned = np.stack([pred_bin[:, bin_names.index(n)] for n in hard_bin_names], axis=1)\n",
        "else:\n",
        "    y_bin_true = (test_df[binary_cols].fillna(0.0).astype(np.float32).values >= 0.5).astype(int)\n",
        "    pred_bin_aligned = pred_bin\n",
        "    hard_bin_names = bin_names\n",
        "\n",
        "y_shade_true = test_df['shade_class'].fillna(0).astype(int).values\n",
        "# stored as 1..5; convert to 0..4 to match training targets\n",
        "y_score_true = test_df['score_class'].fillna(1).astype(int).values - 1\n",
        "y_veg_true   = test_df['veg_class'].fillna(1).astype(int).values - 1\n",
        "\n",
        "# Metrics\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_fscore_support,\n",
        "    accuracy_score,\n",
        "    roc_auc_score,\n",
        "    average_precision_score,\n",
        ")\n",
        "\n",
        "print('--- Binary (threshold=0.5) ---')\n",
        "for i, name in enumerate(hard_bin_names):\n",
        "    y_prob = pred_bin_aligned[:, i]\n",
        "    y_hat = (y_prob >= 0.5).astype(int)\n",
        "    y_true = y_bin_true[:, i]\n",
        "    p, r, f1, _ = precision_recall_fscore_support(y_true, y_hat, average='binary', zero_division=0)\n",
        "    print(f\"{name:24s} P={p:.2f} R={r:.2f} F1={f1:.2f}\")\n",
        "\n",
        "\n",
        "f1_list_05 = []\n",
        "for i, name in enumerate(hard_bin_names):\n",
        "    y_prob = pred_bin_aligned[:, i]\n",
        "    y_hat = (y_prob >= 0.5).astype(int)\n",
        "    y_true = y_bin_true[:, i]\n",
        "    _, _, f1, _ = precision_recall_fscore_support(y_true, y_hat, average='binary', zero_division=0)\n",
        "    f1_list_05.append(float(f1))\n",
        "# Overall overall F1 for threshold=0.5 (binary heads)\n",
        "# Uses the same per-label F1 definition as the printed table above.\n",
        "print(f\"overall F1 (threshold=0.5) = {float(np.mean(f1_list_05)):.3f}\")\n",
        "\n",
        "print('--- Binary (AUC) ---')\n",
        "roc_list = []\n",
        "ap_list = []\n",
        "for i, name in enumerate(hard_bin_names):\n",
        "    y_true = y_bin_true[:, i]\n",
        "    y_prob = pred_bin_aligned[:, i]\n",
        "\n",
        "    if np.unique(y_true).size < 2:\n",
        "        print(f\"{name:24s} ROC_AUC=NA PR_AUC=NA (single-class)\")\n",
        "        continue\n",
        "\n",
        "    roc = float(roc_auc_score(y_true, y_prob))\n",
        "    ap = float(average_precision_score(y_true, y_prob))\n",
        "    roc_list.append(roc)\n",
        "    ap_list.append(ap)\n",
        "    print(f\"{name:24s} ROC_AUC={roc:.3f} PR_AUC={ap:.3f}\")\n",
        "\n",
        "if roc_list:\n",
        "    print(f\"overall ROC_AUC={float(np.mean(roc_list)):.3f} overall PR_AUC={float(np.mean(ap_list)):.3f}\")\n",
        "else:\n",
        "    print(\"overall ROC_AUC=NA overall PR_AUC=NA (no definable labels)\")\n",
        "\n",
        "print('--- Binary (tuned thresholds from val; optimize F1) ---')\n",
        "if 'best_thresholds' not in globals() or not isinstance(best_thresholds, dict) or len(best_thresholds) == 0:\n",
        "    print('No tuned thresholds found. Run the threshold tuning cell above.')\n",
        "else:\n",
        "    f1_list = []\n",
        "    for i, name in enumerate(hard_bin_names):\n",
        "        thr = best_thresholds.get(name, None)\n",
        "        if thr is None or not np.isfinite(thr):\n",
        "            print(f\"{name:24s} thr=NA (not tuned / single-class on val)\")\n",
        "            continue\n",
        "\n",
        "        y_prob = pred_bin_aligned[:, i]\n",
        "        y_hat = (y_prob >= thr).astype(int)\n",
        "        y_true = y_bin_true[:, i]\n",
        "        p, r, f1, _ = precision_recall_fscore_support(y_true, y_hat, average='binary', zero_division=0)\n",
        "        f1_list.append(float(f1))\n",
        "        print(f\"{name:24s} thr={thr:.3f} P={p:.2f} R={r:.2f} F1={f1:.2f}\")\n",
        "\n",
        "    if f1_list:\n",
        "        print(f\"overall F1={float(np.mean(f1_list)):.3f} (over tuned/defined labels)\")\n",
        "    else:\n",
        "        print('overall F1=NA (no definable labels)')\n",
        "\n",
        "print('--- Shade / Score / Veg ---')\n",
        "shade_acc = accuracy_score(y_shade_true, pred_shade.argmax(axis=1))\n",
        "score_acc = accuracy_score(y_score_true, pred_score.argmax(axis=1))\n",
        "veg_acc   = accuracy_score(y_veg_true,   pred_veg.argmax(axis=1))\n",
        "print(f\"Shade accuracy: {shade_acc:.3f}\")\n",
        "print(f\"Score accuracy: {score_acc:.3f}\")\n",
        "print(f\"Veg   accuracy: {veg_acc:.3f}\")\n",
        "\n",
        "# Expected-value MAE for score/veg (convert back to 1..5 scale)\n",
        "classes_1to5 = np.arange(1, 6, dtype=np.float32)\n",
        "score_expected = (pred_score * classes_1to5).sum(axis=1)\n",
        "veg_expected   = (pred_veg   * classes_1to5).sum(axis=1)\n",
        "\n",
        "score_true_1to5 = (y_score_true + 1).astype(np.float32)\n",
        "veg_true_1to5   = (y_veg_true + 1).astype(np.float32)\n",
        "\n",
        "mae_score = float(np.mean(np.abs(score_expected - score_true_1to5)))\n",
        "mae_veg   = float(np.mean(np.abs(veg_expected   - veg_true_1to5)))\n",
        "print(f\"Score MAE (expected value): {mae_score:.3f}\")\n",
        "print(f\"Veg   MAE (expected value): {mae_veg:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Overall metrics ---\n",
            "Binary overall F1@0.5 : 0.516\n",
            "Binary overall ROC_AUC: 0.821\n",
            "Binary overall PR_AUC : 0.625\n",
            "Binary overall F1@tuned: 0.608\n",
            "Saved: /Users/starsrain/2025_codeProject/GreenSpace_CNN/report_outputs/eval_summary_20260204_203132.json\n",
            "Saved: /Users/starsrain/2025_codeProject/GreenSpace_CNN/report_outputs/binary_metrics_20260204_203132.csv\n"
          ]
        }
      ],
      "source": [
        "# Save evaluation artifacts (per run) to report_outputs/\n",
        "# - binary_metrics_<run_tag>.csv : per-label metrics (thr=0.5 + tuned if available + AUC)\n",
        "# - eval_summary_<run_tag>.json  : compact overall summary + provenance\n",
        "\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "out_dir = Path('../report_outputs').resolve()\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "tag = globals().get('run_tag', None) or datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "# --- Binary per-label table ---\n",
        "assert 'hard_bin_names' in globals(), 'Expected hard_bin_names from the evaluation cell.'\n",
        "assert 'pred_bin_aligned' in globals(), 'Expected pred_bin_aligned from the evaluation cell.'\n",
        "assert 'y_bin_true' in globals(), 'Expected y_bin_true from the evaluation cell.'\n",
        "\n",
        "rows = []\n",
        "for i, name in enumerate(hard_bin_names):\n",
        "    y_true = y_bin_true[:, i]\n",
        "    y_prob = pred_bin_aligned[:, i]\n",
        "\n",
        "    # threshold=0.5\n",
        "    y_hat_05 = (y_prob >= 0.5).astype(int)\n",
        "    p05, r05, f105, _ = precision_recall_fscore_support(y_true, y_hat_05, average='binary', zero_division=0)\n",
        "\n",
        "    # AUCs (may be undefined for single-class)\n",
        "    roc = None\n",
        "    ap = None\n",
        "    if np.unique(y_true).size >= 2:\n",
        "        roc = float(roc_auc_score(y_true, y_prob))\n",
        "        ap = float(average_precision_score(y_true, y_prob))\n",
        "\n",
        "    # tuned threshold (if available)\n",
        "    thr = None\n",
        "    pt = rt = f1t = None\n",
        "    if 'best_thresholds' in globals() and isinstance(best_thresholds, dict):\n",
        "        thr = best_thresholds.get(name, None)\n",
        "        if thr is not None and np.isfinite(thr):\n",
        "            y_hat_t = (y_prob >= float(thr)).astype(int)\n",
        "            pt, rt, f1t, _ = precision_recall_fscore_support(y_true, y_hat_t, average='binary', zero_division=0)\n",
        "            pt, rt, f1t = float(pt), float(rt), float(f1t)\n",
        "\n",
        "    rows.append({\n",
        "        'label': name,\n",
        "        'support_pos_test': int(np.sum(y_true == 1)),\n",
        "        'support_neg_test': int(np.sum(y_true == 0)),\n",
        "        'P@0.5': float(p05),\n",
        "        'R@0.5': float(r05),\n",
        "        'F1@0.5': float(f105),\n",
        "        'ROC_AUC': roc,\n",
        "        'PR_AUC': ap,\n",
        "        'tuned_thr': (float(thr) if thr is not None and np.isfinite(thr) else None),\n",
        "        'P@tuned': pt,\n",
        "        'R@tuned': rt,\n",
        "        'F1@tuned': f1t,\n",
        "    })\n",
        "\n",
        "bin_df = pd.DataFrame(rows)\n",
        "\n",
        "# Overall (mean over labels where defined)\n",
        "overall = {\n",
        "    'overall_F1@0.5': float(np.nanmean(bin_df['F1@0.5'].values)) if len(bin_df) else None,\n",
        "    'overall_ROC_AUC': float(np.nanmean(bin_df['ROC_AUC'].values)) if 'ROC_AUC' in bin_df.columns else None,\n",
        "    'overall_PR_AUC': float(np.nanmean(bin_df['PR_AUC'].values)) if 'PR_AUC' in bin_df.columns else None,\n",
        "    'overall_F1@tuned': float(np.nanmean(bin_df['F1@tuned'].values)) if 'F1@tuned' in bin_df.columns else None,\n",
        "}\n",
        "\n",
        "# --- Showcase overall metrics first ---\n",
        "print('--- Overall metrics ---')\n",
        "print(f\"Binary overall F1@0.5 : {overall['overall_F1@0.5']:.3f}\" if overall.get('overall_F1@0.5') is not None else 'Binary overall F1@0.5 : NA')\n",
        "print(f\"Binary overall ROC_AUC: {overall['overall_ROC_AUC']:.3f}\" if overall.get('overall_ROC_AUC') is not None else 'Binary overall ROC_AUC: NA')\n",
        "print(f\"Binary overall PR_AUC : {overall['overall_PR_AUC']:.3f}\" if overall.get('overall_PR_AUC') is not None else 'Binary overall PR_AUC : NA')\n",
        "print(f\"Binary overall F1@tuned: {overall['overall_F1@tuned']:.3f}\" if overall.get('overall_F1@tuned') is not None else 'Binary overall F1@tuned: NA')\n",
        "\n",
        "# --- Save a compact JSON summary (provenance + multiclass + overall) ---\n",
        "summary = {\n",
        "    'run_tag': tag,\n",
        "    'saved_at': datetime.now().isoformat(timespec='seconds'),\n",
        "    'model_path': str(globals().get('model_path', '')),\n",
        "    'run_dir': str(globals().get('RUN_DIR', '')),\n",
        "    'n_test': int(len(y_shade_true)) if 'y_shade_true' in globals() else None,\n",
        "    'binary_overall': overall,\n",
        "    'shade_accuracy': float(globals().get('shade_acc')) if 'shade_acc' in globals() else None,\n",
        "    'score_accuracy': float(globals().get('score_acc')) if 'score_acc' in globals() else None,\n",
        "    'veg_accuracy': float(globals().get('veg_acc')) if 'veg_acc' in globals() else None,\n",
        "    'score_mae_expected_value': float(globals().get('mae_score')) if 'mae_score' in globals() else None,\n",
        "    'veg_mae_expected_value': float(globals().get('mae_veg')) if 'mae_veg' in globals() else None,\n",
        "}\n",
        "\n",
        "summary_path = out_dir / f\"eval_summary_{tag}.json\"\n",
        "summary_path.write_text(json.dumps(summary, indent=2))\n",
        "print('Saved:', summary_path)\n",
        "\n",
        "# --- Then save label-level metrics ---\n",
        "bin_out_path = out_dir / f\"binary_metrics_{tag}.csv\"\n",
        "bin_df.to_csv(bin_out_path, index=False)\n",
        "print('Saved:', bin_out_path)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
