{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 03. Model Training - GreenSpace CNN\n",
        "\n",
        "This notebook trains a multi-task CNN using the manifests produced in 02:\n",
        "- Inputs: `data/processed/splits/{train,val,test}.csv` (paths point to `data/cache/images/`)\n",
        "- Backbone: EfficientNetB0 (ImageNet weights)\n",
        "- Heads:\n",
        "  - Binary features: sigmoid\n",
        "  - Shade: 2-class softmax (minimal/abundant)\n",
        "  - Structured–Unstructured: 5-class softmax\n",
        "  - Vegetation cover distribution: 5-class softmax\n",
        "\n",
        "Baseline in this notebook: **no augmentation** (we can add it back after the first clean run).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/starsrain/2025_codeProject/GreenSpace_CNN/.venv/lib/python3.11/site-packages/keras/src/export/tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
            "  if not hasattr(np, \"object\"):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded splits: 1896 632 632\n"
          ]
        }
      ],
      "source": [
        "# Imports and paths\n",
        "import os\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "\n",
        "# Global reproducibility controls\n",
        "GLOBAL_SEED = 123\n",
        "RNG_STATE_AUG = 123\n",
        "\n",
        "# Set seeds\n",
        "random.seed(GLOBAL_SEED)\n",
        "np.random.seed(GLOBAL_SEED)\n",
        "tf.random.set_seed(GLOBAL_SEED)\n",
        "\n",
        "train_csv = Path('../data/processed/splits/train.csv')\n",
        "val_csv   = Path('../data/processed/splits/val.csv')\n",
        "test_csv  = Path('../data/processed/splits/test.csv')\n",
        "\n",
        "assert train_csv.exists() and val_csv.exists() and test_csv.exists(), 'Missing split manifests. Run 02 first.'\n",
        "\n",
        "train_df = pd.read_csv(train_csv)\n",
        "val_df   = pd.read_csv(val_csv)\n",
        "test_df  = pd.read_csv(test_csv)\n",
        "\n",
        "print('Loaded splits:', len(train_df), len(val_df), len(test_df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary labels: ['sports_field_p', 'multipurpose_open_area_p', 'children_s_playground_p', 'water_feature_p', 'gardens_p', 'walking_paths_p', 'built_structures_p', 'parking_lots_p']\n",
            "Using class cols: {'shade_class': True, 'score_class': True, 'veg_class': True}\n",
            "Datasets ready.\n"
          ]
        }
      ],
      "source": [
        "# Build tf.data datasets from manifests\n",
        "IMG_SIZE = (512, 512)\n",
        "BATCH_SIZE = 8\n",
        "\n",
        "# Binary labels are stored as probabilities in *_p columns\n",
        "binary_cols = [c for c in train_df.columns if c.endswith('_p')]\n",
        "\n",
        "# Shade/Score/Veg in the current split manifests are stored as integer class columns\n",
        "# (not one-hot probability columns like shade_p_*/score_p_*/veg_p_*)\n",
        "HAS_SHADE_CLASS = 'shade_class' in train_df.columns\n",
        "HAS_SCORE_CLASS = 'score_class' in train_df.columns\n",
        "HAS_VEG_CLASS   = 'veg_class' in train_df.columns\n",
        "\n",
        "assert 'image_path' in train_df.columns, \"Missing image_path in split manifests\"\n",
        "assert len(binary_cols) > 0, \"No binary *_p columns found in split manifests\"\n",
        "assert HAS_SHADE_CLASS, \"Missing shade_class in split manifests\"\n",
        "assert HAS_SCORE_CLASS, \"Missing score_class in split manifests\"\n",
        "assert HAS_VEG_CLASS, \"Missing veg_class in split manifests\"\n",
        "\n",
        "print('Binary labels:', binary_cols)\n",
        "print('Using class cols:', {'shade_class': HAS_SHADE_CLASS, 'score_class': HAS_SCORE_CLASS, 'veg_class': HAS_VEG_CLASS})\n",
        "\n",
        "# Configure head sizes + loss modes\n",
        "NUM_SHADE = 2  # minimal vs abundant\n",
        "NUM_SCORE = 5  # 1..5\n",
        "NUM_VEG   = 5  # 1..5\n",
        "\n",
        "SHADE_LOSS_MODE = 'sparse'  # uses shade_class\n",
        "SCORE_LOSS_MODE = 'sparse'  # uses score_class\n",
        "VEG_LOSS_MODE   = 'sparse'  # uses veg_class\n",
        "\n",
        "# Map a row to (image, label dict)\n",
        "def decode_image(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "# Geometry-only augmentation (safe for color-sensitive labels)\n",
        "def augment_geom(img):\n",
        "    # Random 90-degree rotation + flips\n",
        "    k = tf.random.uniform((), minval=0, maxval=4, dtype=tf.int32)\n",
        "    img = tf.image.rot90(img, k)\n",
        "    img = tf.image.random_flip_left_right(img)\n",
        "    img = tf.image.random_flip_up_down(img)\n",
        "    return img\n",
        "\n",
        "# Build a dataset from a DataFrame\n",
        "# - Apply augmentation only on TRAIN (augment=True)\n",
        "# - Never augment val/test\n",
        "# - Cache decoded images to reduce I/O overhead\n",
        "\n",
        "def make_ds(df, shuffle=True, augment=False):\n",
        "    paths = df['image_path'].astype(str).tolist()\n",
        "\n",
        "    # IMPORTANT: fill NaNs to avoid NaN loss during training\n",
        "    y_bin = df[binary_cols].fillna(0.0).astype(np.float32).values\n",
        "\n",
        "    # shade_class should be 0/1; clip defensively\n",
        "    y_shade = df['shade_class'].fillna(0).astype(np.int32).values\n",
        "    y_shade = np.clip(y_shade, 0, NUM_SHADE - 1)\n",
        "\n",
        "    # score_class and veg_class are expected 1..5; convert to 0..4 for sparse CE\n",
        "    y_score = df['score_class'].fillna(1).astype(np.int32).values - 1\n",
        "    y_score = np.clip(y_score, 0, NUM_SCORE - 1)\n",
        "\n",
        "    y_veg = df['veg_class'].fillna(1).astype(np.int32).values - 1\n",
        "    y_veg = np.clip(y_veg, 0, NUM_VEG - 1)\n",
        "\n",
        "    ds_paths = tf.data.Dataset.from_tensor_slices(paths)\n",
        "    ds_imgs = ds_paths.map(decode_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    ds_labels = tf.data.Dataset.from_tensor_slices({\n",
        "        'bin_head': y_bin,\n",
        "        'shade_head': y_shade,\n",
        "        'score_head': y_score,\n",
        "        'veg_head': y_veg,\n",
        "    })\n",
        "\n",
        "    ds = tf.data.Dataset.zip((ds_imgs, ds_labels))\n",
        "\n",
        "    # Shuffle before caching to keep per-epoch order variability\n",
        "    if shuffle and len(paths) > 1:\n",
        "        ds = ds.shuffle(buffer_size=len(paths), seed=GLOBAL_SEED, reshuffle_each_iteration=True)\n",
        "\n",
        "    # Cache decoded images (and labels) to reduce disk I/O; augmentation stays after cache\n",
        "    ds = ds.cache()\n",
        "\n",
        "    if augment:\n",
        "        ds = ds.map(lambda img, y: (augment_geom(img), y), num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    ds = ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_ds = make_ds(train_df, shuffle=True, augment=True)\n",
        "val_ds   = make_ds(val_df, shuffle=False, augment=False)\n",
        "test_ds  = make_ds(test_df, shuffle=False, augment=False)\n",
        "\n",
        "print('Datasets ready.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: save a small augmentation preview grid\n",
        "# This helps verify augmentations are label-preserving and not too aggressive.\n",
        "\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "out_dir = Path('../data/interim/aug_preview').resolve()\n",
        "out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "tag = globals().get('RUN_TAG', None) or datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "# Take a few samples from the training dataset (already augmented)\n",
        "batch_imgs, _ = next(iter(train_ds.take(1)))\n",
        "\n",
        "n = min(int(batch_imgs.shape[0]), 8)\n",
        "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
        "axes = axes.flatten()\n",
        "for i in range(8):\n",
        "    axes[i].axis('off')\n",
        "for i in range(n):\n",
        "    axes[i].imshow(batch_imgs[i].numpy())\n",
        "    axes[i].set_title(f\"aug {i}\")\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "out_path = out_dir / f\"aug_preview_{tag}.png\"\n",
        "plt.savefig(out_path, dpi=150)\n",
        "plt.show()\n",
        "print('Saved augmentation preview to', out_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a multi-head model (EfficientNetB0 backbone)\n",
        "from tensorflow.keras import layers, models, applications, optimizers\n",
        "\n",
        "\n",
        "# IMPORTANT:\n",
        "# Your error occurs *inside* EfficientNetB0(weights='imagenet') before it returns a model,\n",
        "# so prints placed after that line will never run.\n",
        "# We'll do a two-step build:\n",
        "# 1) Build with weights=None and verify the model really expects 3-channel input.\n",
        "# 2) Try to load ImageNet weights; if it fails, we fall back to weights=None so you can keep training.\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "INPUT_SHAPE = (512, 512, 3)\n",
        "print('INPUT_SHAPE used for backbone:', INPUT_SHAPE)\n",
        "assert INPUT_SHAPE[-1] == 3, f\"Expected 3-channel RGB input for imagenet weights, got {INPUT_SHAPE}\"\n",
        "\n",
        "NUM_BIN = len(binary_cols)\n",
        "\n",
        "# These are configured in the dataset cell (based on the current manifest schema)\n",
        "assert NUM_SHADE == 2 and NUM_SCORE == 5 and NUM_VEG == 5, (NUM_SHADE, NUM_SCORE, NUM_VEG)\n",
        "\n",
        "# Explicit input tensor (forces 3-channel model build)\n",
        "inputs = layers.Input(shape=INPUT_SHAPE, name='img')\n",
        "\n",
        "# Step 1: sanity-build WITHOUT weights (cannot fail during weight loading)\n",
        "backbone_no_weights = applications.EfficientNetB0(include_top=False, weights=None, input_tensor=inputs)\n",
        "stem0 = backbone_no_weights.get_layer('stem_conv')\n",
        "print('Sanity stem_conv kernel shape (weights=None):', tuple(stem0.kernel.shape))\n",
        "assert int(stem0.kernel.shape[2]) == 3, (\n",
        "    \"Backbone was built with 1 input channel even though INPUT_SHAPE says 3. \"\n",
        "    \"This usually means a global Keras/TensorFlow config is forcing grayscale/channels.\"\n",
        ")\n",
        "\n",
        "# Step 2: try ImageNet weights\n",
        "try:\n",
        "    backbone = applications.EfficientNetB0(include_top=False, weights='imagenet', input_tensor=inputs)\n",
        "    stem = backbone.get_layer('stem_conv')\n",
        "    print('Loaded ImageNet weights. stem_conv kernel shape:', tuple(stem.kernel.shape))\n",
        "except Exception as e:\n",
        "    print('FAILED to load ImageNet weights for EfficientNetB0:', repr(e))\n",
        "    print('Falling back to weights=None so you can proceed with training.')\n",
        "    print('If you want to fix ImageNet weights loading, try:')\n",
        "    print('  - Delete cached EfficientNet weights in ~/.keras/models/ (files with efficientnetb0...) and rerun')\n",
        "    print('  - Or align package versions: TensorFlow + Keras (mismatched installs can cause weird weight loading)')\n",
        "    backbone = backbone_no_weights\n",
        "\n",
        "x = layers.GlobalAveragePooling2D()(backbone.output)\n",
        "\n",
        "# Heads\n",
        "bin_out = layers.Dense(NUM_BIN, activation='sigmoid', name='bin_head')(x)\n",
        "shade_out = layers.Dense(NUM_SHADE, activation='softmax', name='shade_head')(x)\n",
        "score_out = layers.Dense(NUM_SCORE, activation='softmax', name='score_head')(x)\n",
        "veg_out = layers.Dense(NUM_VEG, activation='softmax', name='veg_head')(x)\n",
        "\n",
        "model = models.Model(inputs=inputs, outputs=[bin_out, shade_out, score_out, veg_out])\n",
        "\n",
        "# Compile (class heads use sparse targets from *_class columns)\n",
        "# Binary head: focal loss with per-label alpha derived from training prevalence.\n",
        "# This often improves PR-AUC / recall on rare labels.\n",
        "\n",
        "# Per-label prevalence in TRAIN (binary_cols are soft probabilities in [0,1])\n",
        "bin_prev = train_df[binary_cols].fillna(0.0).astype(np.float32).mean().values\n",
        "\n",
        "# Alpha per label: rarer labels get higher alpha (more weight on positives)\n",
        "alpha_vec = tf.constant(np.clip(1.0 - bin_prev, 0.25, 0.95), dtype=tf.float32)\n",
        "FOCAL_GAMMA = 2.0\n",
        "_EPS = 1e-7\n",
        "\n",
        "# Optional: print alphas for transparency\n",
        "bin_names = [c[:-2] for c in binary_cols]\n",
        "print('Binary focal alpha per label:')\n",
        "for n, a, p in zip(bin_names, alpha_vec.numpy().tolist(), bin_prev.tolist()):\n",
        "    print(f\"  {n:24s} prevalence={p:.3f} alpha={a:.3f}\")\n",
        "\n",
        "def bin_focal_loss(y_true, y_pred):\n",
        "    # y_true/y_pred: (batch, num_bin)\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.clip_by_value(tf.cast(y_pred, tf.float32), _EPS, 1.0 - _EPS)\n",
        "\n",
        "    # For each element, pick alpha for the true class (per label)\n",
        "    alpha_t = y_true * alpha_vec + (1.0 - y_true) * (1.0 - alpha_vec)\n",
        "\n",
        "    # p_t = prob assigned to the true class\n",
        "    p_t = y_true * y_pred + (1.0 - y_true) * (1.0 - y_pred)\n",
        "\n",
        "    focal = -alpha_t * tf.pow(1.0 - p_t, FOCAL_GAMMA) * tf.math.log(p_t)\n",
        "\n",
        "    # mean over labels, then mean over batch\n",
        "    return tf.reduce_mean(tf.reduce_mean(focal, axis=-1))\n",
        "\n",
        "losses = {\n",
        "    'bin_head': bin_focal_loss,\n",
        "    'shade_head': 'sparse_categorical_crossentropy' if SHADE_LOSS_MODE == 'sparse' else 'categorical_crossentropy',\n",
        "    'score_head': 'sparse_categorical_crossentropy' if SCORE_LOSS_MODE == 'sparse' else 'categorical_crossentropy',\n",
        "    'veg_head': 'sparse_categorical_crossentropy' if VEG_LOSS_MODE == 'sparse' else 'categorical_crossentropy',\n",
        "}\n",
        "metrics = {\n",
        "    'bin_head': ['binary_accuracy'],\n",
        "    'shade_head': ['sparse_categorical_accuracy' if SHADE_LOSS_MODE == 'sparse' else 'accuracy'],\n",
        "    'score_head': ['sparse_categorical_accuracy' if SCORE_LOSS_MODE == 'sparse' else 'accuracy'],\n",
        "    'veg_head': ['sparse_categorical_accuracy' if VEG_LOSS_MODE == 'sparse' else 'accuracy'],\n",
        "}\n",
        "\n",
        "loss_weights_v1 = {\n",
        "    'bin_head': 0.5,\n",
        "    'shade_head': 1.0,\n",
        "    'score_head': 1.5,\n",
        "    'veg_head': 1.5,\n",
        "}  # can change per run\n",
        "\n",
        "loss_weights_v2 = {\n",
        "    'bin_head': 1,\n",
        "    'shade_head': 1.0,\n",
        "    'score_head': 1.2,\n",
        "    'veg_head': 1.2,\n",
        "} \n",
        "\n",
        "model.compile(optimizer=optimizers.Adam(1e-3), loss=losses, metrics=metrics, loss_weights=loss_weights_v2)\n",
        "\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RUN_TAG: 20260129_115813\n",
            "Epoch 1/5\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m511s\u001b[0m 2s/step - bin_head_binary_accuracy: 0.7985 - bin_head_loss: 0.4448 - loss: 4.7972 - score_head_loss: 1.4297 - score_head_sparse_categorical_accuracy: 0.3645 - shade_head_loss: 0.6270 - shade_head_sparse_categorical_accuracy: 0.6582 - veg_head_loss: 1.2022 - veg_head_sparse_categorical_accuracy: 0.4926 - val_bin_head_binary_accuracy: 0.6523 - val_bin_head_loss: 0.8113 - val_loss: 9.3289 - val_score_head_loss: 2.6513 - val_score_head_sparse_categorical_accuracy: 0.1946 - val_shade_head_loss: 1.1539 - val_shade_head_sparse_categorical_accuracy: 0.7104 - val_veg_head_loss: 2.5282 - val_veg_head_sparse_categorical_accuracy: 0.1487 - learning_rate: 0.0010\n",
            "Epoch 2/5\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m479s\u001b[0m 2s/step - bin_head_binary_accuracy: 0.8289 - bin_head_loss: 0.3747 - loss: 4.1717 - score_head_loss: 1.2479 - score_head_sparse_categorical_accuracy: 0.4420 - shade_head_loss: 0.5858 - shade_head_sparse_categorical_accuracy: 0.6957 - veg_head_loss: 1.0178 - veg_head_sparse_categorical_accuracy: 0.5728 - val_bin_head_binary_accuracy: 0.6523 - val_bin_head_loss: 0.7184 - val_loss: 6.4914 - val_score_head_loss: 1.9446 - val_score_head_sparse_categorical_accuracy: 0.2073 - val_shade_head_loss: 0.7821 - val_shade_head_sparse_categorical_accuracy: 0.7104 - val_veg_head_loss: 1.6221 - val_veg_head_sparse_categorical_accuracy: 0.2975 - learning_rate: 0.0010\n",
            "Epoch 3/5\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m471s\u001b[0m 2s/step - bin_head_binary_accuracy: 0.8358 - bin_head_loss: 0.3651 - loss: 3.9023 - score_head_loss: 1.1929 - score_head_sparse_categorical_accuracy: 0.4805 - shade_head_loss: 0.5580 - shade_head_sparse_categorical_accuracy: 0.7083 - veg_head_loss: 0.9149 - veg_head_sparse_categorical_accuracy: 0.6065 - val_bin_head_binary_accuracy: 0.6523 - val_bin_head_loss: 0.6397 - val_loss: 6.3815 - val_score_head_loss: 1.6730 - val_score_head_sparse_categorical_accuracy: 0.2120 - val_shade_head_loss: 0.6425 - val_shade_head_sparse_categorical_accuracy: 0.7104 - val_veg_head_loss: 1.9397 - val_veg_head_sparse_categorical_accuracy: 0.1487 - learning_rate: 0.0010\n",
            "Epoch 4/5\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 2s/step - bin_head_binary_accuracy: 0.8376 - bin_head_loss: 0.3562 - loss: 3.7441 - score_head_loss: 1.1566 - score_head_sparse_categorical_accuracy: 0.4905 - shade_head_loss: 0.5478 - shade_head_sparse_categorical_accuracy: 0.7205 - veg_head_loss: 0.8556 - veg_head_sparse_categorical_accuracy: 0.6577 - val_bin_head_binary_accuracy: 0.6169 - val_bin_head_loss: 0.9798 - val_loss: 13.7537 - val_score_head_loss: 2.9629 - val_score_head_sparse_categorical_accuracy: 0.1978 - val_shade_head_loss: 1.1555 - val_shade_head_sparse_categorical_accuracy: 0.7104 - val_veg_head_loss: 5.1093 - val_veg_head_sparse_categorical_accuracy: 0.0649 - learning_rate: 0.0010\n",
            "Epoch 5/5\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m500s\u001b[0m 2s/step - bin_head_binary_accuracy: 0.8397 - bin_head_loss: 0.3506 - loss: 3.5040 - score_head_loss: 1.0795 - score_head_sparse_categorical_accuracy: 0.5295 - shade_head_loss: 0.5218 - shade_head_sparse_categorical_accuracy: 0.7395 - veg_head_loss: 0.7917 - veg_head_sparse_categorical_accuracy: 0.6799 - val_bin_head_binary_accuracy: 0.6545 - val_bin_head_loss: 0.7094 - val_loss: 6.7958 - val_score_head_loss: 1.9762 - val_score_head_sparse_categorical_accuracy: 0.2057 - val_shade_head_loss: 0.6647 - val_shade_head_sparse_categorical_accuracy: 0.7104 - val_veg_head_loss: 1.8747 - val_veg_head_sparse_categorical_accuracy: 0.1503 - learning_rate: 0.0010\n",
            "Epoch 1/20\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m498s\u001b[0m 2s/step - bin_head_binary_accuracy: 0.8450 - bin_head_loss: 0.3454 - loss: 3.3424 - score_head_loss: 1.0238 - score_head_sparse_categorical_accuracy: 0.5533 - shade_head_loss: 0.5059 - shade_head_sparse_categorical_accuracy: 0.7600 - veg_head_loss: 0.7521 - veg_head_sparse_categorical_accuracy: 0.6941 - val_bin_head_binary_accuracy: 0.7075 - val_bin_head_loss: 0.6057 - val_loss: 6.3580 - val_score_head_loss: 1.7522 - val_score_head_sparse_categorical_accuracy: 0.2706 - val_shade_head_loss: 0.7267 - val_shade_head_sparse_categorical_accuracy: 0.7104 - val_veg_head_loss: 1.8001 - val_veg_head_sparse_categorical_accuracy: 0.1535 - learning_rate: 1.0000e-04\n",
            "Epoch 2/20\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m507s\u001b[0m 2s/step - bin_head_binary_accuracy: 0.8500 - bin_head_loss: 0.3346 - loss: 3.0674 - score_head_loss: 0.9548 - score_head_sparse_categorical_accuracy: 0.5965 - shade_head_loss: 0.4839 - shade_head_sparse_categorical_accuracy: 0.7595 - veg_head_loss: 0.6560 - veg_head_sparse_categorical_accuracy: 0.7511 - val_bin_head_binary_accuracy: 0.6588 - val_bin_head_loss: 0.6830 - val_loss: 6.3898 - val_score_head_loss: 1.7597 - val_score_head_sparse_categorical_accuracy: 0.2405 - val_shade_head_loss: 0.6974 - val_shade_head_sparse_categorical_accuracy: 0.7104 - val_veg_head_loss: 1.8076 - val_veg_head_sparse_categorical_accuracy: 0.1804 - learning_rate: 1.0000e-04\n",
            "Epoch 3/20\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m542s\u001b[0m 2s/step - bin_head_binary_accuracy: 0.8527 - bin_head_loss: 0.3314 - loss: 2.8502 - score_head_loss: 0.8797 - score_head_sparse_categorical_accuracy: 0.6234 - shade_head_loss: 0.4591 - shade_head_sparse_categorical_accuracy: 0.7785 - veg_head_loss: 0.6039 - veg_head_sparse_categorical_accuracy: 0.7648 - val_bin_head_binary_accuracy: 0.7364 - val_bin_head_loss: 0.5244 - val_loss: 6.5773 - val_score_head_loss: 1.5648 - val_score_head_sparse_categorical_accuracy: 0.3544 - val_shade_head_loss: 0.7906 - val_shade_head_sparse_categorical_accuracy: 0.7104 - val_veg_head_loss: 2.1182 - val_veg_head_sparse_categorical_accuracy: 0.1598 - learning_rate: 1.0000e-04\n",
            "Epoch 4/20\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m532s\u001b[0m 2s/step - bin_head_binary_accuracy: 0.8523 - bin_head_loss: 0.3297 - loss: 2.6185 - score_head_loss: 0.8207 - score_head_sparse_categorical_accuracy: 0.6651 - shade_head_loss: 0.4358 - shade_head_sparse_categorical_accuracy: 0.7901 - veg_head_loss: 0.5246 - veg_head_sparse_categorical_accuracy: 0.8075 - val_bin_head_binary_accuracy: 0.7134 - val_bin_head_loss: 0.5724 - val_loss: 5.9708 - val_score_head_loss: 1.5691 - val_score_head_sparse_categorical_accuracy: 0.3386 - val_shade_head_loss: 0.6953 - val_shade_head_sparse_categorical_accuracy: 0.7104 - val_veg_head_loss: 1.7570 - val_veg_head_sparse_categorical_accuracy: 0.1962 - learning_rate: 5.0000e-05\n",
            "Epoch 5/20\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m506s\u001b[0m 2s/step - bin_head_binary_accuracy: 0.8552 - bin_head_loss: 0.3284 - loss: 2.4109 - score_head_loss: 0.7480 - score_head_sparse_categorical_accuracy: 0.6946 - shade_head_loss: 0.4257 - shade_head_sparse_categorical_accuracy: 0.7969 - veg_head_loss: 0.4660 - veg_head_sparse_categorical_accuracy: 0.8307 - val_bin_head_binary_accuracy: 0.8022 - val_bin_head_loss: 0.4265 - val_loss: 4.8610 - val_score_head_loss: 1.4751 - val_score_head_sparse_categorical_accuracy: 0.3687 - val_shade_head_loss: 0.6507 - val_shade_head_sparse_categorical_accuracy: 0.7294 - val_veg_head_loss: 1.1896 - val_veg_head_sparse_categorical_accuracy: 0.4905 - learning_rate: 5.0000e-05\n",
            "Epoch 6/20\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m539s\u001b[0m 2s/step - bin_head_binary_accuracy: 0.8525 - bin_head_loss: 0.3286 - loss: 2.2844 - score_head_loss: 0.7206 - score_head_sparse_categorical_accuracy: 0.7089 - shade_head_loss: 0.4012 - shade_head_sparse_categorical_accuracy: 0.8143 - veg_head_loss: 0.4253 - veg_head_sparse_categorical_accuracy: 0.8465 - val_bin_head_binary_accuracy: 0.7898 - val_bin_head_loss: 0.4537 - val_loss: 5.2722 - val_score_head_loss: 1.4431 - val_score_head_sparse_categorical_accuracy: 0.4161 - val_shade_head_loss: 0.7976 - val_shade_head_sparse_categorical_accuracy: 0.7136 - val_veg_head_loss: 1.3888 - val_veg_head_sparse_categorical_accuracy: 0.4161 - learning_rate: 5.0000e-05\n",
            "Epoch 7/20\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m526s\u001b[0m 2s/step - bin_head_binary_accuracy: 0.8494 - bin_head_loss: 0.3323 - loss: 2.1628 - score_head_loss: 0.6517 - score_head_sparse_categorical_accuracy: 0.7574 - shade_head_loss: 0.3875 - shade_head_sparse_categorical_accuracy: 0.8254 - veg_head_loss: 0.4211 - veg_head_sparse_categorical_accuracy: 0.8534 - val_bin_head_binary_accuracy: 0.7518 - val_bin_head_loss: 0.5203 - val_loss: 5.4589 - val_score_head_loss: 1.6238 - val_score_head_sparse_categorical_accuracy: 0.3639 - val_shade_head_loss: 0.6932 - val_shade_head_sparse_categorical_accuracy: 0.7294 - val_veg_head_loss: 1.3799 - val_veg_head_sparse_categorical_accuracy: 0.4573 - learning_rate: 5.0000e-05\n",
            "Epoch 8/20\n",
            "\u001b[1m237/237\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m509s\u001b[0m 2s/step - bin_head_binary_accuracy: 0.8527 - bin_head_loss: 0.3295 - loss: 1.9642 - score_head_loss: 0.6112 - score_head_sparse_categorical_accuracy: 0.7780 - shade_head_loss: 0.3612 - shade_head_sparse_categorical_accuracy: 0.8397 - veg_head_loss: 0.3477 - veg_head_sparse_categorical_accuracy: 0.8871 - val_bin_head_binary_accuracy: 0.7814 - val_bin_head_loss: 0.4718 - val_loss: 5.5679 - val_score_head_loss: 1.8173 - val_score_head_sparse_categorical_accuracy: 0.3513 - val_shade_head_loss: 0.7382 - val_shade_head_sparse_categorical_accuracy: 0.7184 - val_veg_head_loss: 1.2453 - val_veg_head_sparse_categorical_accuracy: 0.4731 - learning_rate: 2.5000e-05\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "# Train (warm-up then fine-tune)\n",
        "from datetime import datetime\n",
        "\n",
        "# One tag per run so artifacts don't overwrite each other.\n",
        "\n",
        "RUN_TAG = globals().get('RUN_TAG', None) or datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "print('RUN_TAG:', RUN_TAG)\n",
        "\n",
        "EPOCHS_WARMUP = 5\n",
        "EPOCHS_FINETUNE = 20\n",
        "\n",
        "# Warm-up: freeze backbone, train heads\n",
        "for layer in model.layers:\n",
        "    if isinstance(layer, tf.keras.Model) or layer.name.startswith('efficientnet'):\n",
        "        layer.trainable = False\n",
        "\n",
        "ckpt_dir = Path('../models'); ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
        "ckpt_path = ckpt_dir / f'best_{RUN_TAG}.keras'\n",
        "\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath=str(ckpt_path), monitor='val_loss', save_best_only=True, save_weights_only=False),\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2),\n",
        "]\n",
        "\n",
        "# Sanity checks: ensure focal loss + weights are actually in effect\n",
        "assert callable(losses.get('bin_head', None)), f\"Expected bin_head loss to be a function, got: {losses.get('bin_head')}\"\n",
        "\n",
        "print('Sanity: bin_head loss =', losses['bin_head'])\n",
        "\n",
        "\n",
        "history_warmup = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS_WARMUP,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "# Fine-tune: unfreeze top backbone blocks\n",
        "for layer in model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Sanity checks again before fine-tune compile\n",
        "assert callable(losses.get('bin_head', None)), f\"Expected bin_head loss to be a function, got: {losses.get('bin_head')}\"\n",
        "\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(5e-5), loss=losses, metrics=metrics, loss_weights=loss_weights_v2)\n",
        "\n",
        "history_finetune = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS_FINETUNE,\n",
        "    callbacks=callbacks,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "print('Training complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: Evaluation and threshold calibration are now in 04_model_evaluation.ipynb\n"
          ]
        }
      ],
      "source": [
        "# Evaluation moved to 04_model_evaluation.ipynb\n",
        "print('Note: Evaluation and threshold calibration are now in 04_model_evaluation.ipynb')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved final model to ../models/final_20260129_115813.keras\n",
            "Saved weights to ../models/final_20260129_115813.weights.h5\n",
            "Saved model_config_20260129_115813.json\n"
          ]
        }
      ],
      "source": [
        "# Save final artifacts: trained model and config\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# Reuse the same RUN_TAG used for checkpoints (or make one if needed)\n",
        "RUN_TAG = globals().get('RUN_TAG', None) or datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "\n",
        "save_dir = Path('../models'); save_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 1) Save final model (may differ from best checkpoint if last epoch improved)\n",
        "final_path = save_dir / f\"final_{RUN_TAG}.keras\"\n",
        "model.save(str(final_path))\n",
        "print('Saved final model to', final_path)\n",
        "\n",
        "# 2) Save weights separately (also tagged)\n",
        "weights_path = save_dir / f\"final_{RUN_TAG}.weights.h5\"\n",
        "model.save_weights(str(weights_path))\n",
        "print('Saved weights to', weights_path)\n",
        "\n",
        "# 3) Save label/config metadata for inference (also tagged)\n",
        "# NOTE: split manifests store class targets as integer columns:\n",
        "# - shade_class: 0/1\n",
        "# - score_class: 1..5\n",
        "# - veg_class  : 1..5\n",
        "# In training we convert score/veg to 0..4 for sparse categorical losses.\n",
        "config = {\n",
        "    'run_tag': RUN_TAG,\n",
        "    'img_size': IMG_SIZE,\n",
        "    'binary_cols': binary_cols,\n",
        "    'shade_class_col': 'shade_class',\n",
        "    'score_class_col': 'score_class',\n",
        "    'veg_class_col': 'veg_class',\n",
        "    'num_shade': int(NUM_SHADE),\n",
        "    'num_score': int(NUM_SCORE),\n",
        "    'num_veg': int(NUM_VEG),\n",
        "    'score_veg_training_zero_based': True,\n",
        "}\n",
        "\n",
        "config_path = save_dir / f\"model_config_{RUN_TAG}.json\"\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "print('Saved', config_path.name)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "greenspace-venv",
      "language": "python",
      "name": "greenspace-venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
