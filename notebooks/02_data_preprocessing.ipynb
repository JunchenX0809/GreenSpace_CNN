{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02. Data Preprocessing - GreenSpace CNN\n",
        "\n",
        "This notebook handles data preprocessing for the multi-task greenspace CNN:\n",
        "- Survey response cleaning and label processing  \n",
        "- Image preprocessing and augmentation setup\n",
        "- Train/validation/test splits\n",
        "- Data pipeline creation for TensorFlow/Keras\n",
        "\n",
        "## Steps (tentative)\n",
        "1. Load and clean survey data\n",
        "2. Handle multi-rater aggregation \n",
        "3. Process images (resize, normalize)\n",
        "4. Create data splits\n",
        "5. Set up TensorFlow data pipelines\n",
        "\n",
        "How to transform the data we have:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: validate CSV ↔ images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "expected=49 present=49 missing=0 extra=0\n",
            "Alignment OK\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "csv = Path('../data/raw/survey_responses_clean.csv')\n",
        "imgs = Path('../data/raw/images')\n",
        "assert csv.exists() and imgs.exists(), 'Missing CSV or images folder'\n",
        "\n",
        "df = pd.read_csv(csv)\n",
        "assert 'image_filename' in df.columns, \"CSV needs 'image_filename'\"\n",
        "exp = set(df['image_filename'].dropna().astype(str).unique())\n",
        "present = {p.name for p in imgs.iterdir() if p.is_file()}\n",
        "missing = sorted(exp - present)\n",
        "extra = sorted(present - exp)\n",
        "\n",
        "print(f'expected={len(exp)} present={len(present)} missing={len(missing)} extra={len(extra)}')\n",
        "if missing: print('missing (first 10):', *missing[:10], sep='\\n')\n",
        "if extra: print('extra (first 10):', *extra[:10], sep='\\n')\n",
        "\n",
        "if missing or extra:\n",
        "    raise SystemExit('Fix naming/alignment before proceeding.')\n",
        "print('Alignment OK')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: aggregate rater labels → soft & hard per image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote: ../data/processed/labels_soft.csv\n",
            "Wrote: ../data/processed/labels_hard.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "csv = Path('../data/raw/survey_responses_clean.csv')\n",
        "out = Path('../data/processed'); out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "df = pd.read_csv(csv)\n",
        "key = 'image_filename'\n",
        "\n",
        "# Raw column names (as in the CSV)\n",
        "bin_cols_raw = ['Sports Field','Multipurpose Open Area',\"Children's playground\",'Water feature','Gardens','Walking paths','Built structures']\n",
        "shade_col_raw = 'Shade along paths'\n",
        "score_col_raw = 'Structured–Unstructured Rating'\n",
        "\n",
        "# Map to numeric with floats to allow NaN\n",
        "yn = {'Yes':1, 'No':0, 'yes':1, 'no':0}\n",
        "for c in bin_cols_raw:\n",
        "    df[c] = df[c].map(yn).astype(float)\n",
        "\n",
        "shade_map = {'None':0,'Some':1,'Abundant':2}\n",
        "df['shade_i'] = df[shade_col_raw].map(shade_map).astype(float)\n",
        "\n",
        "# Extract leading 1–5 from strings like \"5 - Very Unstructured\"\n",
        "df['score_i'] = pd.to_numeric(df[score_col_raw].astype(str).str.extract(r'^(\\d)')[0], errors='coerce')\n",
        "\n",
        "# Group by image\n",
        "g = df.groupby(key, dropna=False)\n",
        "\n",
        "# n_ratings\n",
        "n = g.size().rename('n_ratings')\n",
        "\n",
        "# Binary soft probs (mean over available ratings)\n",
        "bin_soft = g[bin_cols_raw].mean().rename(columns={\n",
        "    'Sports Field':'sports_field_p',\n",
        "    'Multipurpose Open Area':'multipurpose_open_area_p',\n",
        "    \"Children's playground\":'childrens_playground_p',\n",
        "    'Water feature':'water_feature_p',\n",
        "    'Gardens':'gardens_p',\n",
        "    'Walking paths':'walking_paths_p',\n",
        "    'Built structures':'built_structures_p',\n",
        "}).astype(float)\n",
        "\n",
        "# Shade soft probs\n",
        "shade_probs = g['shade_i'].apply(lambda s: s.value_counts(normalize=True)).unstack(fill_value=0.0)\n",
        "shade_probs = shade_probs.reindex(columns=[0.0,1.0,2.0], fill_value=0.0).astype(float)\n",
        "shade_probs.columns = ['shade_p_none','shade_p_some','shade_p_abundant']\n",
        "\n",
        "# Score soft probs + mean\n",
        "score_probs = g['score_i'].apply(lambda s: s.value_counts(normalize=True)).unstack(fill_value=0.0)\n",
        "score_probs = score_probs.reindex(columns=[1.0,2.0,3.0,4.0,5.0], fill_value=0.0).astype(float)\n",
        "score_probs.columns = [f'score_p_{i}' for i in [1,2,3,4,5]]\n",
        "score_mean = g['score_i'].mean().rename('score_mean').astype(float)\n",
        "\n",
        "# Assemble soft labels\n",
        "soft = pd.concat([n, bin_soft, shade_probs, score_probs, score_mean], axis=1).reset_index()\n",
        "soft.to_csv(out / 'labels_soft.csv', index=False)\n",
        "\n",
        "# Build hard labels from soft (fill NaNs with 0 before argmax/threshold)\n",
        "hard = soft[[key,'n_ratings']].copy()\n",
        "for col in ['sports_field','multipurpose_open_area','childrens_playground','water_feature','gardens','walking_paths','built_structures']:\n",
        "    hard[col] = soft[f'{col}_p'].fillna(0.0).ge(0.5).astype(int)\n",
        "\n",
        "shade_cols = ['shade_p_none','shade_p_some','shade_p_abundant']\n",
        "shade_arr = soft[shade_cols].fillna(0.0).to_numpy(dtype=float)\n",
        "hard['shade_class'] = shade_arr.argmax(axis=1)  # 0/1/2\n",
        "\n",
        "score_cols = [f'score_p_{i}' for i in [1,2,3,4,5]]\n",
        "score_arr = soft[score_cols].fillna(0.0).to_numpy(dtype=float)\n",
        "hard['score_class'] = score_arr.argmax(axis=1) + 1  # 1..5\n",
        "\n",
        "hard.to_csv(out / 'labels_hard.csv', index=False)\n",
        "\n",
        "print('Wrote:', out/'labels_soft.csv')\n",
        "print('Wrote:', out/'labels_hard.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: label prevalence (from soft labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary prevalence (expected positive rate):\n",
            "                          prevalence\n",
            "gardens_p                      0.088\n",
            "childrens_playground_p         0.112\n",
            "water_feature_p                0.192\n",
            "sports_field_p                 0.218\n",
            "built_structures_p             0.327\n",
            "multipurpose_open_area_p       0.456\n",
            "walking_paths_p                0.567\n",
            "\n",
            "Shade distribution:\n",
            "           prob\n",
            "none      0.000\n",
            "some      0.925\n",
            "abundant  0.075\n",
            "\n",
            "Structured–Unstructured (1–5) distribution:\n",
            "          prob\n",
            "score_1  0.133\n",
            "score_2  0.168\n",
            "score_3  0.128\n",
            "score_4  0.236\n",
            "score_5  0.335\n",
            "\n",
            "Number of ratings per image (summary):\n",
            "count    49.000000\n",
            "mean      2.448980\n",
            "std       1.415115\n",
            "min       1.000000\n",
            "25%       2.000000\n",
            "50%       2.000000\n",
            "75%       3.000000\n",
            "max       8.000000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "soft_path = Path('../data/processed/labels_soft.csv')\n",
        "assert soft_path.exists(), f'Missing {soft_path}. Run Step 2 first.'\n",
        "\n",
        "df = pd.read_csv(soft_path)\n",
        "\n",
        "# Binary prevalence (mean probability = expected positive rate)\n",
        "binary_cols = [c for c in df.columns if c.endswith('_p') and not c.startswith(('shade_p_', 'score_p_'))]\n",
        "bin_prev = df[binary_cols].mean().sort_values()\n",
        "print('Binary prevalence (expected positive rate):')\n",
        "print(bin_prev.to_frame('prevalence').round(3))\n",
        "\n",
        "# Shade (3-class) distribution (dataset-level)\n",
        "shade_cols = ['shade_p_none','shade_p_some','shade_p_abundant']\n",
        "shade_prev = df[shade_cols].mean()\n",
        "print('\\nShade distribution:')\n",
        "print(shade_prev.rename(lambda c: c.replace('shade_p_','')).to_frame('prob').round(3))\n",
        "\n",
        "# Structured–Unstructured (1–5) distribution\n",
        "score_cols = [f'score_p_{i}' for i in [1,2,3,4,5]]\n",
        "score_prev = df[score_cols].mean()\n",
        "print('\\nStructured–Unstructured (1–5) distribution:')\n",
        "print(score_prev.rename(lambda c: c.replace('score_p_','score_')).to_frame('prob').round(3))\n",
        "\n",
        "# Rating count sanity check\n",
        "if 'n_ratings' in df.columns:\n",
        "    print('\\nNumber of ratings per image (summary):')\n",
        "    print(df['n_ratings'].describe().to_string())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Augmentation Ideas\n",
        "\n",
        "- Geometric: horizontal/vertical flips, 90° rotations, random crop→resize (90–100% area), small translate (±5%).\n",
        "- Photometric: brightness/contrast ±10% (keep mild since “shade” is label-like). Avoid heavy color/hue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: oversample (rare vs other) + rotation/brightness augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rare_labels=['sports_field', 'childrens_playground', 'water_feature', 'gardens']\n",
            "rare=26 other=23\n",
            "Oversampled + augmented stream ready (weights=[0.5, 0.5]).\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "hard_p = Path('../data/processed/labels_hard.csv')\n",
        "soft_p = Path('../data/processed/labels_soft.csv')\n",
        "img_dir = Path('../data/raw/images')\n",
        "\n",
        "dh = pd.read_csv(hard_p)\n",
        "ds = pd.read_csv(soft_p)\n",
        "# Join to keep flexibility later (we only need hard to detect rarity)\n",
        "df = dh.merge(ds, on=['image_filename','n_ratings'], how='inner')\n",
        "\n",
        "df['image_path'] = df['image_filename'].apply(lambda x: str(img_dir / x))\n",
        "\n",
        "# Dynamically choose rare labels from soft prevalence\n",
        "soft_probs = [c for c in ds.columns if c.endswith('_p') and not c.startswith(('shade_p_', 'score_p_'))]\n",
        "soft_prev = ds[soft_probs].mean()\n",
        "rare_threshold = 0.25\n",
        "# Strip only a trailing '_p' (avoid removing the '_p' inside names like 'childrens_playground_p')\n",
        "strip_p = lambda s: s[:-2] if s.endswith('_p') else s\n",
        "rare_labels = [strip_p(c) for c, v in soft_prev.items() if v <= rare_threshold]\n",
        "if not rare_labels:\n",
        "    rare_labels = [strip_p(c) for c in soft_prev.sort_values().head(2).index]\n",
        "\n",
        "# Mark images positive for any rare label\n",
        "df['is_rare_pos'] = df[[*rare_labels]].any(axis=1)\n",
        "\n",
        "rare_paths = df.loc[df['is_rare_pos'], 'image_path'].tolist()\n",
        "other_paths = df.loc[~df['is_rare_pos'], 'image_path'].tolist()\n",
        "print(f'rare_labels={rare_labels}')\n",
        "print(f'rare={len(rare_paths)} other={len(other_paths)}')\n",
        "\n",
        "# Dynamic weights with safe fallbacks\n",
        "target_rare = 0.5\n",
        "if len(rare_paths) == 0 and len(other_paths) == 0:\n",
        "    weights = [0.5, 0.5]\n",
        "elif len(rare_paths) == 0:\n",
        "    weights = [0.0, 1.0]\n",
        "elif len(other_paths) == 0:\n",
        "    weights = [1.0, 0.0]\n",
        "else:\n",
        "    weights = [target_rare, 1.0 - target_rare]\n",
        "\n",
        "IMG_SIZE = (512, 512)\n",
        "\n",
        "def decode(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "# Mild, label-preserving augmentation: rotate 0/90/180/270 and brightness ±10%\n",
        "def augment(img):\n",
        "    k = tf.random.uniform((), minval=0, maxval=4, dtype=tf.int32)\n",
        "    img = tf.image.rot90(img, k)\n",
        "    delta = tf.random.uniform((), minval=-0.1, maxval=0.1)\n",
        "    img = tf.clip_by_value(img + delta, 0.0, 1.0)\n",
        "    return img\n",
        "    # try to save the augmented images. \n",
        "\n",
        "def make_ds(paths, augment_flag=True, shuffle_seed=123):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(paths)\n",
        "    if len(paths) > 1:\n",
        "        ds = ds.shuffle(len(paths), seed=shuffle_seed, reshuffle_each_iteration=True)\n",
        "    ds = ds.map(decode, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    if augment_flag:\n",
        "        ds = ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "rare_ds = make_ds(rare_paths, augment_flag=True, shuffle_seed=123)\n",
        "other_ds = make_ds(other_paths, augment_flag=True, shuffle_seed=456)\n",
        "\n",
        "# Oversampled mix with dynamic weights\n",
        "mixed = tf.data.Dataset.sample_from_datasets([rare_ds, other_ds], weights=weights, seed=999)\n",
        "train_preview = mixed.batch(8).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print(f'Oversampled + augmented stream ready (weights={weights}).')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: dynamic 60/20/20 split → train/val/test manifests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split sizes: 29 10 10\n",
            "Rare labels used: ['sports_field', 'childrens_playground', 'water_feature', 'gardens']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "soft_csv = Path('../data/processed/labels_soft.csv')\n",
        "hard_csv = Path('../data/processed/labels_hard.csv')\n",
        "img_dir = Path('../data/raw/images')\n",
        "out_dir = Path('../data/processed/splits'); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "soft = pd.read_csv(soft_csv)\n",
        "hard = pd.read_csv(hard_csv)\n",
        "\n",
        "# Merge soft and hard; keep one row per image\n",
        "df = hard.merge(soft, on=['image_filename','n_ratings'], how='inner')\n",
        "\n",
        "# Build absolute image_path for convenience\n",
        "df['image_path'] = df['image_filename'].apply(lambda x: str((img_dir / x).resolve()))\n",
        "\n",
        "# Identify columns\n",
        "soft_prob_cols = [c for c in df.columns if c.endswith('_p')]\n",
        "soft_prob_cols = [c for c in soft_prob_cols if not c.startswith(('shade_p_','score_p_'))] + \\\n",
        "                  [c for c in df.columns if c.startswith('shade_p_')] + \\\n",
        "                  [c for c in df.columns if c.startswith('score_p_')]\n",
        "\n",
        "hard_bin_cols = [c for c in ['sports_field','multipurpose_open_area','childrens_playground','water_feature','gardens','walking_paths','built_structures'] if c in df.columns]\n",
        "other_hard_cols = [c for c in ['shade_class','score_class'] if c in df.columns]\n",
        "keep_cols = ['image_path','image_filename','n_ratings'] + soft_prob_cols + hard_bin_cols + other_hard_cols\n",
        "\n",
        "# Define rare labels dynamically from soft prevalence (for stratification hints)\n",
        "soft_prev = df[[c for c in df.columns if c.endswith('_p') and not c.startswith(('shade_p_','score_p_'))]].mean()\n",
        "strip_p = lambda s: s[:-2] if s.endswith('_p') else s\n",
        "rare_labels = [strip_p(c) for c, v in soft_prev.items() if v <= 0.25]\n",
        "if not rare_labels:\n",
        "    rare_labels = [strip_p(c) for c in soft_prev.sort_values().head(2).index]\n",
        "\n",
        "# Stratify proxy: number of rare positives (clipped) per image\n",
        "stratum = df[[c for c in rare_labels if c in df.columns]].sum(axis=1)\n",
        "stratum = stratum.clip(upper=2)  # 0,1,2\n",
        "\n",
        "# Compute dynamic sizes\n",
        "N = len(df)\n",
        "train_ratio, val_ratio, test_ratio = 0.6, 0.2, 0.2\n",
        "train_size = max(1, round(train_ratio * N))\n",
        "val_size   = max(1, round(val_ratio * N))\n",
        "# Adjust to sum to N\n",
        "test_size  = max(1, N - train_size - val_size)\n",
        "if train_size + val_size + test_size != N:\n",
        "    # Fix rounding drift by adjusting test_size\n",
        "    test_size = N - train_size - val_size\n",
        "\n",
        "# First split: train vs temp (val+test)\n",
        "train_df, temp_df = train_test_split(\n",
        "    df, train_size=train_size, random_state=123, stratify=stratum if stratum.nunique()>1 else None)\n",
        "\n",
        "# Second split: val vs test from temp\n",
        "temp_stratum = temp_df[[*rare_labels]].sum(axis=1).clip(upper=2) if rare_labels else None\n",
        "val_prop = val_size / max(1, len(temp_df))\n",
        "val_df, test_df = train_test_split(\n",
        "    temp_df, test_size=(1 - val_prop), random_state=456,\n",
        "    stratify=temp_stratum if temp_stratum is not None and temp_stratum.nunique()>1 else None)\n",
        "\n",
        "# Save manifests\n",
        "train_df[keep_cols].to_csv(out_dir / 'train.csv', index=False)\n",
        "val_df[keep_cols].to_csv(out_dir / 'val.csv', index=False)\n",
        "test_df[keep_cols].to_csv(out_dir / 'test.csv', index=False)\n",
        "\n",
        "print('Split sizes:', len(train_df), len(val_df), len(test_df))\n",
        "print('Rare labels used:', rare_labels)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
