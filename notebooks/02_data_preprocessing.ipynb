{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02. Data Preprocessing - GreenSpace CNN\n",
        "\n",
        "This notebook handles data preprocessing for the multi-task greenspace CNN:\n",
        "- Survey response cleaning and label processing  \n",
        "- Image preprocessing and augmentation setup\n",
        "- Train/validation/test splits\n",
        "- Data pipeline creation for TensorFlow/Keras\n",
        "\n",
        "## Steps (tentative)\n",
        "1. Load and clean survey data\n",
        "2. Handle multi-rater aggregation \n",
        "3. Process images (resize, normalize)\n",
        "4. Create data splits\n",
        "5. Set up TensorFlow data pipelines\n",
        "\n",
        "How to transform the data we have:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 1: validate CSV ↔ images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "expected=48 present=48 missing=0 extra=0\n",
            "Alignment OK\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "csv = Path('../data/raw/survey_responses_clean.csv')\n",
        "imgs = Path('../data/raw/images')\n",
        "assert csv.exists() and imgs.exists(), 'Missing CSV or images folder'\n",
        "\n",
        "df = pd.read_csv(csv)\n",
        "assert 'image_filename' in df.columns, \"CSV needs 'image_filename'\"\n",
        "exp = set(df['image_filename'].dropna().astype(str).unique())\n",
        "present = {p.name for p in imgs.iterdir() if p.is_file()}\n",
        "missing = sorted(exp - present)\n",
        "extra = sorted(present - exp)\n",
        "\n",
        "print(f'expected={len(exp)} present={len(present)} missing={len(missing)} extra={len(extra)}')\n",
        "if missing: print('missing (first 10):', *missing[:10], sep='\\n')\n",
        "if extra: print('extra (first 10):', *extra[:10], sep='\\n')\n",
        "\n",
        "if missing or extra:\n",
        "    raise SystemExit('Fix naming/alignment before proceeding.')\n",
        "print('Alignment OK')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: aggregate rater labels → soft & hard per image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote: ../data/processed/labels_soft.csv\n",
            "Wrote: ../data/processed/labels_hard.csv\n"
          ]
        }
      ],
      "source": [
        "\n",
        "csv = Path('../data/raw/survey_responses_clean.csv')\n",
        "out = Path('../data/processed'); out.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "df = pd.read_csv(csv)\n",
        "key = 'image_filename'\n",
        "\n",
        "# Raw column names (as in the CSV)\n",
        "bin_cols_raw = ['Sports Field','Multipurpose Open Area',\"Children's playground\",'Water feature','Gardens','Walking paths','Built structures']\n",
        "shade_col_raw = 'Shade along paths'\n",
        "score_col_raw = 'Structured–Unstructured Rating'\n",
        "\n",
        "# Map to numeric with floats to allow NaN\n",
        "yn = {'Yes':1, 'No':0, 'yes':1, 'no':0}\n",
        "for c in bin_cols_raw:\n",
        "    df[c] = df[c].map(yn).astype(float)\n",
        "\n",
        "shade_map = {'None':0,'Some':1,'Abundant':2}\n",
        "df['shade_i'] = df[shade_col_raw].map(shade_map).astype(float)\n",
        "\n",
        "# Extract leading 1–5 from strings like \"5 - Very Unstructured\"\n",
        "df['score_i'] = pd.to_numeric(df[score_col_raw].astype(str).str.extract(r'^(\\d)')[0], errors='coerce')\n",
        "\n",
        "# Group by image\n",
        "g = df.groupby(key, dropna=False)\n",
        "\n",
        "# n_ratings\n",
        "n = g.size().rename('n_ratings')\n",
        "\n",
        "# Binary soft probs (mean over available ratings)\n",
        "bin_soft = g[bin_cols_raw].mean().rename(columns={\n",
        "    'Sports Field':'sports_field_p',\n",
        "    'Multipurpose Open Area':'multipurpose_open_area_p',\n",
        "    \"Children's playground\":'childrens_playground_p',\n",
        "    'Water feature':'water_feature_p',\n",
        "    'Gardens':'gardens_p',\n",
        "    'Walking paths':'walking_paths_p',\n",
        "    'Built structures':'built_structures_p',\n",
        "}).astype(float)\n",
        "\n",
        "# Shade soft probs\n",
        "shade_probs = g['shade_i'].apply(lambda s: s.value_counts(normalize=True)).unstack(fill_value=0.0)\n",
        "shade_probs = shade_probs.reindex(columns=[0.0,1.0,2.0], fill_value=0.0).astype(float)\n",
        "shade_probs.columns = ['shade_p_none','shade_p_some','shade_p_abundant']\n",
        "\n",
        "# Score soft probs + mean\n",
        "score_probs = g['score_i'].apply(lambda s: s.value_counts(normalize=True)).unstack(fill_value=0.0)\n",
        "score_probs = score_probs.reindex(columns=[1.0,2.0,3.0,4.0,5.0], fill_value=0.0).astype(float)\n",
        "score_probs.columns = [f'score_p_{i}' for i in [1,2,3,4,5]]\n",
        "score_mean = g['score_i'].mean().rename('score_mean').astype(float)\n",
        "\n",
        "# Assemble soft labels\n",
        "soft = pd.concat([n, bin_soft, shade_probs, score_probs, score_mean], axis=1).reset_index()\n",
        "soft.to_csv(out / 'labels_soft.csv', index=False)\n",
        "\n",
        "# Build hard labels from soft (fill NaNs with 0 before argmax/threshold)\n",
        "hard = soft[[key,'n_ratings']].copy()\n",
        "for col in ['sports_field','multipurpose_open_area','childrens_playground','water_feature','gardens','walking_paths','built_structures']:\n",
        "    hard[col] = soft[f'{col}_p'].fillna(0.0).ge(0.5).astype(int)\n",
        "\n",
        "shade_cols = ['shade_p_none','shade_p_some','shade_p_abundant']\n",
        "shade_arr = soft[shade_cols].fillna(0.0).to_numpy(dtype=float)\n",
        "hard['shade_class'] = shade_arr.argmax(axis=1)  # 0/1/2\n",
        "\n",
        "score_cols = [f'score_p_{i}' for i in [1,2,3,4,5]]\n",
        "score_arr = soft[score_cols].fillna(0.0).to_numpy(dtype=float)\n",
        "hard['score_class'] = score_arr.argmax(axis=1) + 1  # 1..5\n",
        "\n",
        "hard.to_csv(out / 'labels_hard.csv', index=False)\n",
        "\n",
        "print('Wrote:', out/'labels_soft.csv')\n",
        "print('Wrote:', out/'labels_hard.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: label prevalence (from soft labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Binary prevalence (expected positive rate):\n",
            "                          prevalence\n",
            "gardens_p                      0.104\n",
            "childrens_playground_p         0.125\n",
            "water_feature_p                0.163\n",
            "sports_field_p                 0.222\n",
            "built_structures_p             0.330\n",
            "multipurpose_open_area_p       0.438\n",
            "walking_paths_p                0.542\n",
            "\n",
            "Shade distribution:\n",
            "           prob\n",
            "none      0.000\n",
            "some      0.972\n",
            "abundant  0.028\n",
            "\n",
            "Structured–Unstructured (1–5) distribution:\n",
            "          prob\n",
            "score_1  0.101\n",
            "score_2  0.160\n",
            "score_3  0.142\n",
            "score_4  0.205\n",
            "score_5  0.392\n",
            "\n",
            "Number of ratings per image (summary):\n",
            "count    48.000000\n",
            "mean      1.708333\n",
            "std       0.742576\n",
            "min       1.000000\n",
            "25%       1.000000\n",
            "50%       2.000000\n",
            "75%       2.000000\n",
            "max       3.000000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "soft_path = Path('../data/processed/labels_soft.csv')\n",
        "assert soft_path.exists(), f'Missing {soft_path}. Run Step 2 first.'\n",
        "\n",
        "df = pd.read_csv(soft_path)\n",
        "\n",
        "# Binary prevalence (mean probability = expected positive rate)\n",
        "bin_cols = [\n",
        "    'sports_field_p','multipurpose_open_area_p','childrens_playground_p',\n",
        "    'water_feature_p','gardens_p','walking_paths_p','built_structures_p'\n",
        "]\n",
        "bin_prev = df[bin_cols].mean().sort_values()\n",
        "print('Binary prevalence (expected positive rate):')\n",
        "print(bin_prev.to_frame('prevalence').round(3))\n",
        "\n",
        "# Shade (3-class) distribution (dataset-level)\n",
        "shade_cols = ['shade_p_none','shade_p_some','shade_p_abundant']\n",
        "shade_prev = df[shade_cols].mean()\n",
        "print('\\nShade distribution:')\n",
        "print(shade_prev.rename(lambda c: c.replace('shade_p_','')).to_frame('prob').round(3))\n",
        "\n",
        "# Structured–Unstructured (1–5) distribution\n",
        "score_cols = [f'score_p_{i}' for i in [1,2,3,4,5]]\n",
        "score_prev = df[score_cols].mean()\n",
        "print('\\nStructured–Unstructured (1–5) distribution:')\n",
        "print(score_prev.rename(lambda c: c.replace('score_p_','score_')).to_frame('prob').round(3))\n",
        "\n",
        "# Rating count sanity check\n",
        "if 'n_ratings' in df.columns:\n",
        "    print('\\nNumber of ratings per image (summary):')\n",
        "    print(df['n_ratings'].describe().to_string())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Augmentation Ideas\n",
        "\n",
        "- Geometric: horizontal/vertical flips, 90° rotations, random crop→resize (90–100% area), small translate (±5%).\n",
        "- Photometric: brightness/contrast ±10% (keep mild since “shade” is label-like). Avoid heavy color/hue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: oversample (rare vs other) + rotation/brightness augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rare=24 other=24\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-09-08 20:34:01.156328: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4\n",
            "2025-09-08 20:34:01.156359: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
            "2025-09-08 20:34:01.156367: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
            "2025-09-08 20:34:01.156406: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
            "2025-09-08 20:34:01.156421: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Oversampled + augmented stream ready (preview dataset built).\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "hard_p = Path('../data/processed/labels_hard.csv')\n",
        "soft_p = Path('../data/processed/labels_soft.csv')\n",
        "img_dir = Path('../data/raw/images')\n",
        "\n",
        "dh = pd.read_csv(hard_p)\n",
        "ds = pd.read_csv(soft_p)\n",
        "# Join to keep flexibility later (we only need hard to detect rarity)\n",
        "df = dh.merge(ds, on=['image_filename','n_ratings'], how='inner')\n",
        "\n",
        "df['image_path'] = df['image_filename'].apply(lambda x: str(img_dir / x))\n",
        "rare_labels = ['gardens','childrens_playground','water_feature','sports_field']\n",
        "df['is_rare_pos'] = df[rare_labels].any(axis=1)\n",
        "\n",
        "rare_paths = df.loc[df['is_rare_pos'], 'image_path'].tolist()\n",
        "other_paths = df.loc[~df['is_rare_pos'], 'image_path'].tolist()\n",
        "print(f'rare={len(rare_paths)} other={len(other_paths)}')\n",
        "\n",
        "IMG_SIZE = (512, 512)\n",
        "\n",
        "def decode(path):\n",
        "    img = tf.io.read_file(path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMG_SIZE)\n",
        "    img = tf.cast(img, tf.float32) / 255.0\n",
        "    return img\n",
        "\n",
        "# Mild, label-preserving augmentation: rotate 0/90/180/270 and brightness ±10%\n",
        "def augment(img):\n",
        "    k = tf.random.uniform((), minval=0, maxval=4, dtype=tf.int32)\n",
        "    img = tf.image.rot90(img, k)\n",
        "    delta = tf.random.uniform((), minval=-0.1, maxval=0.1)\n",
        "    img = tf.clip_by_value(img + delta, 0.0, 1.0)\n",
        "    return img\n",
        "\n",
        "def make_ds(paths, augment_flag=True, shuffle_seed=123):\n",
        "    ds = tf.data.Dataset.from_tensor_slices(paths)\n",
        "    if len(paths) > 1:\n",
        "        ds = ds.shuffle(len(paths), seed=shuffle_seed, reshuffle_each_iteration=True)\n",
        "    ds = ds.map(decode, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    if augment_flag:\n",
        "        ds = ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "rare_ds = make_ds(rare_paths, augment_flag=True, shuffle_seed=123)\n",
        "other_ds = make_ds(other_paths, augment_flag=True, shuffle_seed=456)\n",
        "\n",
        "# 50/50 oversampled mix\n",
        "mixed = tf.data.Dataset.sample_from_datasets([rare_ds, other_ds], weights=[0.5, 0.5], seed=999)\n",
        "train_preview = mixed.batch(8).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "print('Oversampled + augmented stream ready (preview dataset built).')\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
